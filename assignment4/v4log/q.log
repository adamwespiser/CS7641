2019-04-08 05:57:27,224 - __main__ - INFO - Creating MDPs
2019-04-08 05:57:27,224 - __main__ - INFO - ----------
/home/adam/projects/CS7641/assignment4/.venv/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.
  result = entry_point.load(False)
2019-04-08 05:57:27,311 - __main__ - INFO - Frozen Lake (20x20): State space: 400, Action space: 4
2019-04-08 05:57:27,311 - __main__ - INFO - Frozen Lake (8x8): State space: 64, Action space: 4
2019-04-08 05:57:27,311 - __main__ - INFO - Cliff Walking (4x12): State space: 48, Action space: 4
2019-04-08 05:57:27,311 - __main__ - INFO - ----------
2019-04-08 05:57:27,311 - __main__ - INFO - Running experiments
2019-04-08 05:57:27,311 - __main__ - INFO - Running Q experiment: Frozen Lake (20x20)
2019-04-08 05:57:27,312 - experiments.base - INFO - Searching Q in 216 dimensions
2019-04-08 05:57:27,312 - experiments.base - INFO - 1/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 05:58:44,736 - experiments.base - INFO - Took 2000 episodes
2019-04-08 05:59:54,466 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 05:59:54,481 - experiments.base - INFO - 2/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:00:42,350 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:01:22,890 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:01:22,903 - experiments.base - INFO - 3/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:02:19,693 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:02:59,144 - experiments.base - INFO - reward_mean: 0.011991498645336907, reward_median: 0.010222222222222223, reward_std: 0.014256020649401435, reward_max: 0.08718446601941748, reward_min: -0.01, runs: 2000
2019-04-08 06:02:59,158 - experiments.base - INFO - 4/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 06:03:16,100 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:03:57,636 - experiments.base - INFO - reward_mean: -0.010608452245827585, reward_median: -0.010566037735849056, reward_std: 0.001872789920355679, reward_max: 0.07138211382113821, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 06:03:57,653 - experiments.base - INFO - 5/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 06:04:07,744 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:04:36,966 - experiments.base - INFO - reward_mean: -0.01386116925018346, reward_median: -0.01225, reward_std: 0.004500497138980705, reward_max: -0.010312499999999999, reward_min: -0.0325, runs: 2000
2019-04-08 06:04:36,992 - experiments.base - INFO - 6/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 06:04:51,207 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:05:36,441 - experiments.base - INFO - reward_mean: -0.017472529150556887, reward_median: -0.014500000000000002, reward_std: 0.006634775553625762, reward_max: -0.0104147465437788, reward_min: -0.0325, runs: 2000
2019-04-08 06:05:36,467 - experiments.base - INFO - 7/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 06:05:49,717 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:06:34,683 - experiments.base - INFO - reward_mean: -0.0180290488532002, reward_median: -0.015625, reward_std: 0.006538297991097018, reward_max: -0.01044334975369458, reward_min: -0.0325, runs: 2000
2019-04-08 06:06:34,708 - experiments.base - INFO - 8/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 06:06:47,085 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:07:30,464 - experiments.base - INFO - reward_mean: -0.01813237127653696, reward_median: -0.016428571428571428, reward_std: 0.006428232106522858, reward_max: -0.010282131661442005, reward_min: -0.0325, runs: 2000
2019-04-08 06:07:30,478 - experiments.base - INFO - 9/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 06:07:37,873 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:08:03,753 - experiments.base - INFO - reward_mean: -0.018331100439133333, reward_median: -0.016428571428571428, reward_std: 0.006289198595253032, reward_max: -0.010548780487804879, reward_min: -0.0325, runs: 2000
2019-04-08 06:08:03,768 - experiments.base - INFO - 10/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 06:08:11,070 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:08:44,915 - experiments.base - INFO - reward_mean: -0.01819499527323895, reward_median: -0.016428571428571428, reward_std: 0.006351179549995224, reward_max: -0.010352941176470587, reward_min: -0.0325, runs: 2000
2019-04-08 06:08:44,930 - experiments.base - INFO - 11/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 06:08:52,132 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:09:17,853 - experiments.base - INFO - reward_mean: -0.01837183766677646, reward_median: -0.016923076923076923, reward_std: 0.006152028879971164, reward_max: -0.010309278350515464, reward_min: -0.0325, runs: 2000
2019-04-08 06:09:17,868 - experiments.base - INFO - 12/216 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 06:09:25,012 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:09:50,643 - experiments.base - INFO - reward_mean: -0.018629630336368712, reward_median: -0.017499999999999998, reward_std: 0.005886646303809809, reward_max: -0.010459183673469387, reward_min: -0.0325, runs: 2000
2019-04-08 06:09:50,659 - experiments.base - INFO - 13/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 06:11:08,216 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:11:56,274 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:11:56,282 - experiments.base - INFO - 14/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:12:36,542 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:13:16,723 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:13:16,731 - experiments.base - INFO - 15/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:13:47,705 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:14:18,199 - experiments.base - INFO - reward_mean: 0.025938699370086263, reward_median: 0.02182837933474876, reward_std: 0.01757146250063654, reward_max: 0.12527027027027027, reward_min: -0.01, runs: 2000
2019-04-08 06:14:18,214 - experiments.base - INFO - 16/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 06:14:35,265 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:15:03,261 - experiments.base - INFO - reward_mean: -0.010648193222944883, reward_median: -0.010542168674698794, reward_std: 0.000408885537241858, reward_max: -0.010099557522123893, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 06:15:03,278 - experiments.base - INFO - 17/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 06:15:13,248 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:15:39,838 - experiments.base - INFO - reward_mean: -0.011683733708855065, reward_median: -0.011384615384615384, reward_std: 0.0011320477232685967, reward_max: -0.010246575342465751, reward_min: -0.02125, runs: 2000
2019-04-08 06:15:39,854 - experiments.base - INFO - 18/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 06:15:48,028 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:16:13,726 - experiments.base - INFO - reward_mean: -0.01708979062796651, reward_median: -0.014090909090909093, reward_std: 0.006920626719817397, reward_max: -0.01013081395348837, reward_min: -0.0325, runs: 2000
2019-04-08 06:16:13,741 - experiments.base - INFO - 19/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 06:16:21,460 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:16:47,426 - experiments.base - INFO - reward_mean: -0.016878270809433882, reward_median: -0.015294117647058824, reward_std: 0.005581557239330229, reward_max: -0.0102200488997555, reward_min: -0.0325, runs: 2000
2019-04-08 06:16:47,442 - experiments.base - INFO - 20/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 06:16:54,899 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:17:20,164 - experiments.base - INFO - reward_mean: -0.018539926350390458, reward_median: -0.016923076923076923, reward_std: 0.006253693103366881, reward_max: -0.010535714285714285, reward_min: -0.0325, runs: 2000
2019-04-08 06:17:20,179 - experiments.base - INFO - 21/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 06:17:27,435 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:17:52,605 - experiments.base - INFO - reward_mean: -0.018582219000533744, reward_median: -0.016923076923076923, reward_std: 0.006396705094631792, reward_max: -0.010368852459016394, reward_min: -0.0325, runs: 2000
2019-04-08 06:17:52,620 - experiments.base - INFO - 22/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 06:17:59,854 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:18:24,968 - experiments.base - INFO - reward_mean: -0.017745562889281516, reward_median: -0.016, reward_std: 0.005567175505423696, reward_max: -0.010426540284360188, reward_min: -0.0325, runs: 2000
2019-04-08 06:18:24,982 - experiments.base - INFO - 23/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 06:18:31,913 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:18:57,199 - experiments.base - INFO - reward_mean: -0.018562543004478408, reward_median: -0.016923076923076923, reward_std: 0.006353163307909434, reward_max: -0.010318021201413428, reward_min: -0.0325, runs: 2000
2019-04-08 06:18:57,214 - experiments.base - INFO - 24/216 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 06:19:03,858 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:19:28,759 - experiments.base - INFO - reward_mean: -0.01758910534918652, reward_median: -0.016, reward_std: 0.005962998558822656, reward_max: -0.01036, reward_min: -0.0325, runs: 2000
2019-04-08 06:19:28,771 - experiments.base - INFO - 25/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 06:20:12,694 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:20:52,427 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:20:52,436 - experiments.base - INFO - 26/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:21:31,463 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:22:11,586 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:22:11,594 - experiments.base - INFO - 27/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:22:41,628 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:23:11,976 - experiments.base - INFO - reward_mean: 0.022942974845495692, reward_median: 0.020379432624113476, reward_std: 0.014778546956093225, reward_max: 0.11833333333333333, reward_min: -0.01, runs: 2000
2019-04-08 06:23:11,991 - experiments.base - INFO - 28/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 06:23:28,407 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:23:56,219 - experiments.base - INFO - reward_mean: -0.010509274814343526, reward_median: -0.010478723404255317, reward_std: 0.0010934391839152265, reward_max: 0.025750000000000002, reward_min: -0.012727272727272728, runs: 2000
2019-04-08 06:23:56,237 - experiments.base - INFO - 29/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 06:24:06,069 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:24:31,458 - experiments.base - INFO - reward_mean: -0.012651819095023267, reward_median: -0.012045454545454547, reward_std: 0.002151587213568698, reward_max: -0.010249307479224375, reward_min: -0.025000000000000005, runs: 2000
2019-04-08 06:24:31,474 - experiments.base - INFO - 30/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 06:24:39,825 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:25:05,587 - experiments.base - INFO - reward_mean: -0.01642232778244736, reward_median: -0.013333333333333332, reward_std: 0.00631803236168857, reward_max: -0.010298013245033112, reward_min: -0.0325, runs: 2000
2019-04-08 06:25:05,603 - experiments.base - INFO - 31/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 06:25:13,993 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:25:40,777 - experiments.base - INFO - reward_mean: -0.01771395300892455, reward_median: -0.015000000000000001, reward_std: 0.006497598749061336, reward_max: -0.010412844036697245, reward_min: -0.0325, runs: 2000
2019-04-08 06:25:40,796 - experiments.base - INFO - 32/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 06:25:48,238 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:26:13,957 - experiments.base - INFO - reward_mean: -0.01804967231776706, reward_median: -0.016, reward_std: 0.006049664701612775, reward_max: -0.010452261306532661, reward_min: -0.0325, runs: 2000
2019-04-08 06:26:13,971 - experiments.base - INFO - 33/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 06:26:21,231 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:26:46,299 - experiments.base - INFO - reward_mean: -0.018246099689222212, reward_median: -0.016428571428571428, reward_std: 0.006099829475898761, reward_max: -0.010357142857142858, reward_min: -0.0325, runs: 2000
2019-04-08 06:26:46,314 - experiments.base - INFO - 34/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 06:26:53,400 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:27:19,476 - experiments.base - INFO - reward_mean: -0.018678591360031967, reward_median: -0.017499999999999998, reward_std: 0.005936167698001045, reward_max: -0.01044334975369458, reward_min: -0.0325, runs: 2000
2019-04-08 06:27:19,491 - experiments.base - INFO - 35/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 06:27:26,439 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:27:51,860 - experiments.base - INFO - reward_mean: -0.018142411904698647, reward_median: -0.016214285714285716, reward_std: 0.006541006382976049, reward_max: -0.010468749999999999, reward_min: -0.0325, runs: 2000
2019-04-08 06:27:51,876 - experiments.base - INFO - 36/216 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 06:27:58,777 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:28:23,645 - experiments.base - INFO - reward_mean: -0.018376063181799265, reward_median: -0.016923076923076923, reward_std: 0.005758298489094116, reward_max: -0.010714285714285714, reward_min: -0.0325, runs: 2000
2019-04-08 06:28:23,659 - experiments.base - INFO - 37/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 06:29:09,075 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:29:49,476 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:29:49,485 - experiments.base - INFO - 38/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:30:31,599 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:31:12,547 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:31:12,555 - experiments.base - INFO - 39/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:31:45,260 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:32:16,057 - experiments.base - INFO - reward_mean: 0.023773710813303296, reward_median: 0.020942886901349232, reward_std: 0.014102413281957923, reward_max: 0.0901, reward_min: -0.01, runs: 2000
2019-04-08 06:32:16,072 - experiments.base - INFO - 40/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 06:32:33,377 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:33:00,433 - experiments.base - INFO - reward_mean: -0.010735272643463929, reward_median: -0.010756302521008404, reward_std: 0.0032665568089003606, reward_max: 0.07629310344827586, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 06:33:00,450 - experiments.base - INFO - 41/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 06:33:10,769 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:33:37,340 - experiments.base - INFO - reward_mean: -0.01213390982360626, reward_median: -0.011551724137931036, reward_std: 0.0017169216310744505, reward_max: -0.010139968895800933, reward_min: -0.02125, runs: 2000
2019-04-08 06:33:37,356 - experiments.base - INFO - 42/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 06:33:45,812 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:34:11,253 - experiments.base - INFO - reward_mean: -0.017672442688874145, reward_median: -0.016, reward_std: 0.006323517707513396, reward_max: -0.010309278350515464, reward_min: -0.0325, runs: 2000
2019-04-08 06:34:11,267 - experiments.base - INFO - 43/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 06:34:18,954 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:34:44,301 - experiments.base - INFO - reward_mean: -0.018344656143522552, reward_median: -0.016923076923076923, reward_std: 0.006010140137363986, reward_max: -0.010559006211180123, reward_min: -0.0325, runs: 2000
2019-04-08 06:34:44,316 - experiments.base - INFO - 44/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 06:34:51,820 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:35:17,351 - experiments.base - INFO - reward_mean: -0.018578761864294618, reward_median: -0.016923076923076923, reward_std: 0.0061464526965619186, reward_max: -0.010559006211180123, reward_min: -0.0325, runs: 2000
2019-04-08 06:35:17,366 - experiments.base - INFO - 45/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 06:35:24,506 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:35:49,785 - experiments.base - INFO - reward_mean: -0.018432362334837193, reward_median: -0.016923076923076923, reward_std: 0.006039999836297269, reward_max: -0.010494505494505493, reward_min: -0.0325, runs: 2000
2019-04-08 06:35:49,799 - experiments.base - INFO - 46/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 06:35:57,394 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:36:22,352 - experiments.base - INFO - reward_mean: -0.01890470150103217, reward_median: -0.017499999999999998, reward_std: 0.0058942363988764845, reward_max: -0.010093457943925233, reward_min: -0.0325, runs: 2000
2019-04-08 06:36:22,367 - experiments.base - INFO - 47/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 06:36:29,287 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:36:54,006 - experiments.base - INFO - reward_mean: -0.018495723802742666, reward_median: -0.016428571428571428, reward_std: 0.006163068055530687, reward_max: -0.010511363636363636, reward_min: -0.0325, runs: 2000
2019-04-08 06:36:54,021 - experiments.base - INFO - 48/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 06:37:00,825 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:37:25,555 - experiments.base - INFO - reward_mean: -0.018991512048477502, reward_median: -0.017499999999999998, reward_std: 0.005861168090516506, reward_max: -0.010676691729323307, reward_min: -0.0325, runs: 2000
2019-04-08 06:37:25,571 - experiments.base - INFO - 49/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 06:38:11,460 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:38:51,670 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:38:51,679 - experiments.base - INFO - 50/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:39:32,021 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:40:12,189 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:40:12,198 - experiments.base - INFO - 51/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:40:43,754 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:41:14,788 - experiments.base - INFO - reward_mean: 0.015591864490155902, reward_median: 0.014807968700095816, reward_std: 0.021554322203420247, reward_max: 0.11060240963855421, reward_min: -0.011058823529411763, runs: 2000
2019-04-08 06:41:14,805 - experiments.base - INFO - 52/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 06:41:31,562 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:41:59,088 - experiments.base - INFO - reward_mean: -0.010460344805942249, reward_median: -0.010491803278688525, reward_std: 0.002747790297420823, reward_max: 0.07780701754385966, reward_min: -0.013214285714285715, runs: 2000
2019-04-08 06:41:59,105 - experiments.base - INFO - 53/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 06:42:09,112 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:42:34,403 - experiments.base - INFO - reward_mean: -0.01487675030277564, reward_median: -0.012195121951219513, reward_std: 0.005598522170620756, reward_max: -0.010196936542669585, reward_min: -0.0325, runs: 2000
2019-04-08 06:42:34,417 - experiments.base - INFO - 54/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 06:42:43,197 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:43:09,054 - experiments.base - INFO - reward_mean: -0.017713700622850593, reward_median: -0.015625, reward_std: 0.0061963625176795906, reward_max: -0.0103, reward_min: -0.0325, runs: 2000
2019-04-08 06:43:09,068 - experiments.base - INFO - 55/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 06:43:16,744 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:43:41,736 - experiments.base - INFO - reward_mean: -0.017669876469436473, reward_median: -0.015625, reward_std: 0.0063381141433562615, reward_max: -0.01038793103448276, reward_min: -0.0325, runs: 2000
2019-04-08 06:43:41,750 - experiments.base - INFO - 56/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 06:43:49,135 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:44:14,347 - experiments.base - INFO - reward_mean: -0.01863743473874438, reward_median: -0.017499999999999998, reward_std: 0.006084168895096711, reward_max: -0.010382978723404256, reward_min: -0.0325, runs: 2000
2019-04-08 06:44:14,363 - experiments.base - INFO - 57/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 06:44:21,598 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:44:46,868 - experiments.base - INFO - reward_mean: -0.018387752992594514, reward_median: -0.016428571428571428, reward_std: 0.006195315745066812, reward_max: -0.01045685279187817, reward_min: -0.0325, runs: 2000
2019-04-08 06:44:46,883 - experiments.base - INFO - 58/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 06:44:53,784 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:45:18,594 - experiments.base - INFO - reward_mean: -0.018363193844897385, reward_median: -0.016428571428571428, reward_std: 0.006099530702011196, reward_max: -0.010428571428571426, reward_min: -0.0325, runs: 2000
2019-04-08 06:45:18,609 - experiments.base - INFO - 59/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 06:45:25,547 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:45:50,527 - experiments.base - INFO - reward_mean: -0.018106457959983176, reward_median: -0.016, reward_std: 0.006206305860470938, reward_max: -0.010671641791044777, reward_min: -0.0325, runs: 2000
2019-04-08 06:45:50,541 - experiments.base - INFO - 60/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 06:45:57,388 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:46:22,895 - experiments.base - INFO - reward_mean: -0.018729457682547114, reward_median: -0.016923076923076923, reward_std: 0.006011138024306531, reward_max: -0.010676691729323307, reward_min: -0.0325, runs: 2000
2019-04-08 06:46:22,910 - experiments.base - INFO - 61/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 06:47:06,689 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:47:47,000 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:47:47,008 - experiments.base - INFO - 62/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:48:27,226 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:49:06,996 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:49:07,005 - experiments.base - INFO - 63/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:49:37,903 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:50:10,109 - experiments.base - INFO - reward_mean: 0.0029715454393429157, reward_median: -0.01, reward_std: 0.016997463929526627, reward_max: 0.0931958762886598, reward_min: -0.011084337349397589, runs: 2000
2019-04-08 06:50:10,125 - experiments.base - INFO - 64/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 06:50:26,768 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:50:54,000 - experiments.base - INFO - reward_mean: -0.009786634632637282, reward_median: -0.010555555555555556, reward_std: 0.008289960886846389, reward_max: 0.144, reward_min: -0.01375, runs: 2000
2019-04-08 06:50:54,016 - experiments.base - INFO - 65/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 06:51:04,004 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:51:29,976 - experiments.base - INFO - reward_mean: -0.011484701313936, reward_median: -0.011285714285714286, reward_std: 0.002723173720418875, reward_max: 0.10375000000000001, reward_min: -0.016428571428571428, runs: 2000
2019-04-08 06:51:29,993 - experiments.base - INFO - 66/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 06:51:38,244 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:52:02,529 - experiments.base - INFO - reward_mean: -0.016058032980357056, reward_median: -0.01375, reward_std: 0.005609637158135456, reward_max: -0.010499999999999999, reward_min: -0.0325, runs: 2000
2019-04-08 06:52:02,543 - experiments.base - INFO - 67/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 06:52:10,043 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:52:34,200 - experiments.base - INFO - reward_mean: -0.018334275711452228, reward_median: -0.016428571428571428, reward_std: 0.006190036609165448, reward_max: -0.010489130434782607, reward_min: -0.0325, runs: 2000
2019-04-08 06:52:34,214 - experiments.base - INFO - 68/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 06:52:41,441 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:53:06,400 - experiments.base - INFO - reward_mean: -0.0178939845701565, reward_median: -0.016428571428571428, reward_std: 0.006307501486683301, reward_max: -0.010218446601941746, reward_min: -0.0325, runs: 2000
2019-04-08 06:53:06,416 - experiments.base - INFO - 69/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 06:53:13,735 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:53:38,237 - experiments.base - INFO - reward_mean: -0.01838790965396739, reward_median: -0.016428571428571428, reward_std: 0.006100619804131154, reward_max: -0.010580645161290323, reward_min: -0.0325, runs: 2000
2019-04-08 06:53:38,251 - experiments.base - INFO - 70/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 06:53:45,285 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:54:10,098 - experiments.base - INFO - reward_mean: -0.017280581329728414, reward_median: -0.015294117647058824, reward_std: 0.005582052268351224, reward_max: -0.010865384615384614, reward_min: -0.0325, runs: 2000
2019-04-08 06:54:10,113 - experiments.base - INFO - 71/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 06:54:17,166 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:54:41,341 - experiments.base - INFO - reward_mean: -0.01905507816938365, reward_median: -0.017499999999999998, reward_std: 0.0059064327880178295, reward_max: -0.01079646017699115, reward_min: -0.0325, runs: 2000
2019-04-08 06:54:41,357 - experiments.base - INFO - 72/216 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 06:54:48,172 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:55:12,158 - experiments.base - INFO - reward_mean: -0.018501350450859235, reward_median: -0.016923076923076923, reward_std: 0.00584976263867616, reward_max: -0.010692307692307694, reward_min: -0.0325, runs: 2000
2019-04-08 06:55:12,173 - experiments.base - INFO - 73/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 06:56:06,899 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:56:46,333 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:56:46,342 - experiments.base - INFO - 74/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 06:57:37,157 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:58:16,574 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:58:16,583 - experiments.base - INFO - 75/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 06:59:05,662 - experiments.base - INFO - Took 2000 episodes
2019-04-08 06:59:45,288 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 06:59:45,296 - experiments.base - INFO - 76/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:00:12,918 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:00:41,191 - experiments.base - INFO - reward_mean: 0.03718629724485555, reward_median: 0.03429203539823009, reward_std: 0.017746778276238724, reward_max: 0.13507246376811594, reward_min: 0.0007982740021574985, runs: 2000
2019-04-08 07:00:41,208 - experiments.base - INFO - 77/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:00:50,217 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:01:15,166 - experiments.base - INFO - reward_mean: -0.01820772320633764, reward_median: -0.016923076923076923, reward_std: 0.006643590515361981, reward_max: 0.08813725490196078, reward_min: -0.0325, runs: 2000
2019-04-08 07:01:15,181 - experiments.base - INFO - 78/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:01:21,730 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:01:46,901 - experiments.base - INFO - reward_mean: -0.019074310670908652, reward_median: -0.017499999999999998, reward_std: 0.005942546533908118, reward_max: -0.01045, reward_min: -0.0325, runs: 2000
2019-04-08 07:01:46,915 - experiments.base - INFO - 79/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:01:53,248 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:02:17,854 - experiments.base - INFO - reward_mean: -0.01892978788841719, reward_median: -0.017499999999999998, reward_std: 0.005604494923392907, reward_max: -0.010466321243523313, reward_min: -0.0325, runs: 2000
2019-04-08 07:02:17,870 - experiments.base - INFO - 80/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:02:24,258 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:02:49,098 - experiments.base - INFO - reward_mean: -0.0184392209602142, reward_median: -0.016428571428571428, reward_std: 0.005939092356836151, reward_max: -0.010676691729323307, reward_min: -0.0325, runs: 2000
2019-04-08 07:02:49,113 - experiments.base - INFO - 81/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 07:02:55,296 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:03:19,313 - experiments.base - INFO - reward_mean: -0.018673425520793298, reward_median: -0.016923076923076923, reward_std: 0.006000457484522627, reward_max: -0.010562499999999999, reward_min: -0.0325, runs: 2000
2019-04-08 07:03:19,328 - experiments.base - INFO - 82/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 07:03:25,634 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:03:50,922 - experiments.base - INFO - reward_mean: -0.01859372813981519, reward_median: -0.016428571428571428, reward_std: 0.005904892549059216, reward_max: -0.010656934306569343, reward_min: -0.0325, runs: 2000
2019-04-08 07:03:50,938 - experiments.base - INFO - 83/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 07:03:57,207 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:04:21,903 - experiments.base - INFO - reward_mean: -0.017758591824035597, reward_median: -0.016, reward_std: 0.005543358397813154, reward_max: -0.010588235294117647, reward_min: -0.0325, runs: 2000
2019-04-08 07:04:21,918 - experiments.base - INFO - 84/216 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 07:04:28,168 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:04:52,290 - experiments.base - INFO - reward_mean: -0.01865944525529139, reward_median: -0.016923076923076923, reward_std: 0.005895168483924907, reward_max: -0.010978260869565217, reward_min: -0.0325, runs: 2000
2019-04-08 07:04:52,304 - experiments.base - INFO - 85/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 07:05:45,883 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:06:25,838 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:06:25,846 - experiments.base - INFO - 86/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 07:07:15,792 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:07:56,341 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:07:56,349 - experiments.base - INFO - 87/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 07:08:44,032 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:09:24,213 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:09:24,222 - experiments.base - INFO - 88/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:09:52,196 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:10:20,618 - experiments.base - INFO - reward_mean: 0.0370691022440796, reward_median: 0.03352173913043478, reward_std: 0.01829599272373421, reward_max: 0.1656140350877193, reward_min: 0.005120845921450152, runs: 2000
2019-04-08 07:10:20,634 - experiments.base - INFO - 89/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:10:29,508 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:10:54,926 - experiments.base - INFO - reward_mean: -0.01642888491200136, reward_median: -0.014500000000000002, reward_std: 0.005933872397118412, reward_max: -0.010192307692307691, reward_min: -0.0325, runs: 2000
2019-04-08 07:10:54,941 - experiments.base - INFO - 90/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:11:01,549 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:11:26,158 - experiments.base - INFO - reward_mean: -0.01896426815244631, reward_median: -0.017499999999999998, reward_std: 0.005921243997703528, reward_max: -0.01069767441860465, reward_min: -0.0325, runs: 2000
2019-04-08 07:11:26,173 - experiments.base - INFO - 91/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:11:32,631 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:11:57,811 - experiments.base - INFO - reward_mean: -0.01894312228512131, reward_median: -0.017499999999999998, reward_std: 0.00570213779857182, reward_max: -0.010517241379310345, reward_min: -0.0325, runs: 2000
2019-04-08 07:11:57,825 - experiments.base - INFO - 92/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:12:04,256 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:12:28,698 - experiments.base - INFO - reward_mean: -0.0187735038511942, reward_median: -0.017499999999999998, reward_std: 0.0059274928657048675, reward_max: -0.010620689655172414, reward_min: -0.0325, runs: 2000
2019-04-08 07:12:28,712 - experiments.base - INFO - 93/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 07:12:34,854 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:12:59,459 - experiments.base - INFO - reward_mean: -0.018405568526590404, reward_median: -0.016428571428571428, reward_std: 0.005781816040074636, reward_max: -0.010638297872340425, reward_min: -0.0325, runs: 2000
2019-04-08 07:12:59,474 - experiments.base - INFO - 94/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 07:13:05,677 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:13:30,050 - experiments.base - INFO - reward_mean: -0.019017610981278552, reward_median: -0.017499999999999998, reward_std: 0.0058628667495942855, reward_max: -0.010523255813953488, reward_min: -0.0325, runs: 2000
2019-04-08 07:13:30,065 - experiments.base - INFO - 95/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 07:13:36,141 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:14:01,691 - experiments.base - INFO - reward_mean: -0.017996765925542908, reward_median: -0.016428571428571428, reward_std: 0.005278283461376347, reward_max: -0.010476190476190476, reward_min: -0.0325, runs: 2000
2019-04-08 07:14:01,707 - experiments.base - INFO - 96/216 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 07:14:07,818 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:14:32,003 - experiments.base - INFO - reward_mean: -0.018984438297430656, reward_median: -0.017499999999999998, reward_std: 0.005659749231694552, reward_max: -0.010671641791044777, reward_min: -0.0325, runs: 2000
2019-04-08 07:14:32,018 - experiments.base - INFO - 97/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 07:15:26,166 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:16:05,312 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:16:05,320 - experiments.base - INFO - 98/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 07:16:53,045 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:17:33,181 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:17:33,189 - experiments.base - INFO - 99/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 07:18:18,477 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:18:56,994 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:18:57,003 - experiments.base - INFO - 100/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:19:24,442 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:19:52,136 - experiments.base - INFO - reward_mean: 0.03514616303498957, reward_median: 0.03223628691983122, reward_std: 0.018658258371358892, reward_max: 0.13098591549295777, reward_min: -0.01, runs: 2000
2019-04-08 07:19:52,151 - experiments.base - INFO - 101/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:20:00,603 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:20:25,245 - experiments.base - INFO - reward_mean: -0.01772641867913202, reward_median: -0.016, reward_std: 0.006582162885475034, reward_max: -0.010223325062034737, reward_min: -0.0325, runs: 2000
2019-04-08 07:20:25,259 - experiments.base - INFO - 102/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:20:31,852 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:20:55,982 - experiments.base - INFO - reward_mean: -0.01911297732800766, reward_median: -0.017499999999999998, reward_std: 0.005976450230921615, reward_max: -0.010599999999999998, reward_min: -0.0325, runs: 2000
2019-04-08 07:20:55,997 - experiments.base - INFO - 103/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:21:02,365 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:21:26,800 - experiments.base - INFO - reward_mean: -0.018920359709175685, reward_median: -0.017499999999999998, reward_std: 0.005893871644657508, reward_max: -0.010681818181818183, reward_min: -0.0325, runs: 2000
2019-04-08 07:21:26,815 - experiments.base - INFO - 104/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:21:33,049 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:21:57,503 - experiments.base - INFO - reward_mean: -0.018757512686403778, reward_median: -0.016923076923076923, reward_std: 0.005889525575661479, reward_max: -0.010652173913043479, reward_min: -0.0325, runs: 2000
2019-04-08 07:21:57,518 - experiments.base - INFO - 105/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 07:22:04,088 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:22:28,630 - experiments.base - INFO - reward_mean: -0.018795422892350075, reward_median: -0.016923076923076923, reward_std: 0.005693161685644095, reward_max: -0.010612244897959184, reward_min: -0.0325, runs: 2000
2019-04-08 07:22:28,645 - experiments.base - INFO - 106/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 07:22:34,784 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:22:59,503 - experiments.base - INFO - reward_mean: -0.017589846352609045, reward_median: -0.015625, reward_std: 0.005528016098906764, reward_max: -0.010526315789473684, reward_min: -0.0325, runs: 2000
2019-04-08 07:22:59,517 - experiments.base - INFO - 107/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 07:23:05,716 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:23:30,302 - experiments.base - INFO - reward_mean: -0.015828382399583174, reward_median: -0.014285714285714287, reward_std: 0.004795248627880751, reward_max: -0.010592105263157894, reward_min: -0.0325, runs: 2000
2019-04-08 07:23:30,317 - experiments.base - INFO - 108/216 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 07:23:36,366 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:24:00,723 - experiments.base - INFO - reward_mean: -0.018444315431621155, reward_median: -0.016428571428571428, reward_std: 0.005941689348436202, reward_max: -0.010616438356164382, reward_min: -0.0325, runs: 2000
2019-04-08 07:24:00,738 - experiments.base - INFO - 109/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 07:24:54,894 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:25:34,381 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:25:34,390 - experiments.base - INFO - 110/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 07:26:24,112 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:27:03,242 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:27:03,251 - experiments.base - INFO - 111/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 07:27:51,461 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:28:30,964 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:28:30,972 - experiments.base - INFO - 112/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:28:59,595 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:29:28,133 - experiments.base - INFO - reward_mean: 0.0369690440311974, reward_median: 0.03352173913043478, reward_std: 0.018190034512048033, reward_max: 0.13940298507462687, reward_min: 0.0008804347826086952, runs: 2000
2019-04-08 07:29:28,151 - experiments.base - INFO - 113/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:29:40,423 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:30:05,723 - experiments.base - INFO - reward_mean: -0.013652248575901783, reward_median: -0.011374125874125875, reward_std: 0.0052742685296968675, reward_max: -0.010199115044247786, reward_min: -0.0325, runs: 2000
2019-04-08 07:30:05,738 - experiments.base - INFO - 114/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:30:12,187 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:30:36,677 - experiments.base - INFO - reward_mean: -0.01898339295694455, reward_median: -0.017499999999999998, reward_std: 0.005971802926811069, reward_max: -0.010588235294117647, reward_min: -0.0325, runs: 2000
2019-04-08 07:30:36,693 - experiments.base - INFO - 115/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:30:42,845 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:31:06,992 - experiments.base - INFO - reward_mean: -0.01874580802464652, reward_median: -0.016923076923076923, reward_std: 0.005764719496977271, reward_max: -0.01028125, reward_min: -0.0325, runs: 2000
2019-04-08 07:31:07,006 - experiments.base - INFO - 116/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:31:13,155 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:31:37,577 - experiments.base - INFO - reward_mean: -0.018902089358539202, reward_median: -0.016923076923076923, reward_std: 0.005914951197820203, reward_max: -0.010604026845637585, reward_min: -0.0325, runs: 2000
2019-04-08 07:31:37,591 - experiments.base - INFO - 117/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 07:31:43,815 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:32:08,048 - experiments.base - INFO - reward_mean: -0.018540152798699024, reward_median: -0.016428571428571428, reward_std: 0.0059746513407946044, reward_max: -0.010703125, reward_min: -0.0325, runs: 2000
2019-04-08 07:32:08,062 - experiments.base - INFO - 118/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 07:32:14,220 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:32:38,717 - experiments.base - INFO - reward_mean: -0.017406365222291845, reward_median: -0.015625, reward_std: 0.005354401569753281, reward_max: -0.01038793103448276, reward_min: -0.0325, runs: 2000
2019-04-08 07:32:38,732 - experiments.base - INFO - 119/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 07:32:44,748 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:33:08,834 - experiments.base - INFO - reward_mean: -0.01882693801903131, reward_median: -0.017499999999999998, reward_std: 0.005888435238071159, reward_max: -0.010687022900763359, reward_min: -0.0325, runs: 2000
2019-04-08 07:33:08,848 - experiments.base - INFO - 120/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 07:33:14,872 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:33:39,112 - experiments.base - INFO - reward_mean: -0.018495917305296906, reward_median: -0.016, reward_std: 0.006279256700958263, reward_max: -0.01042056074766355, reward_min: -0.0325, runs: 2000
2019-04-08 07:33:39,127 - experiments.base - INFO - 121/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 07:34:31,824 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:35:11,131 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:35:11,140 - experiments.base - INFO - 122/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 07:35:59,031 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:36:37,811 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:36:37,819 - experiments.base - INFO - 123/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 07:37:24,044 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:38:03,729 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:38:03,737 - experiments.base - INFO - 124/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:38:30,476 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:38:58,342 - experiments.base - INFO - reward_mean: 0.03560255560033744, reward_median: 0.03259574468085106, reward_std: 0.01779048542535324, reward_max: 0.11833333333333333, reward_min: 0.0022521419828641404, runs: 2000
2019-04-08 07:38:58,359 - experiments.base - INFO - 125/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:39:10,014 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:39:35,264 - experiments.base - INFO - reward_mean: -0.01145826949169054, reward_median: -0.011232876712328766, reward_std: 0.001683633666087624, reward_max: 0.051036585365853655, reward_min: -0.017499999999999998, runs: 2000
2019-04-08 07:39:35,281 - experiments.base - INFO - 126/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:39:41,914 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:40:06,969 - experiments.base - INFO - reward_mean: -0.01776802398235383, reward_median: -0.016, reward_std: 0.005642134638284606, reward_max: -0.010569620253164557, reward_min: -0.0325, runs: 2000
2019-04-08 07:40:06,984 - experiments.base - INFO - 127/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:40:13,309 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:40:37,643 - experiments.base - INFO - reward_mean: -0.019025967263734317, reward_median: -0.017499999999999998, reward_std: 0.005711369405148132, reward_max: -0.010439024390243901, reward_min: -0.0325, runs: 2000
2019-04-08 07:40:37,659 - experiments.base - INFO - 128/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:40:43,886 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:41:08,306 - experiments.base - INFO - reward_mean: -0.01696202428283439, reward_median: -0.015294117647058824, reward_std: 0.005066150561805627, reward_max: -0.010599999999999998, reward_min: -0.0325, runs: 2000
2019-04-08 07:41:08,320 - experiments.base - INFO - 129/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 07:41:14,573 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:41:38,924 - experiments.base - INFO - reward_mean: -0.019002675409319207, reward_median: -0.017499999999999998, reward_std: 0.005711303530474948, reward_max: -0.010476190476190476, reward_min: -0.0325, runs: 2000
2019-04-08 07:41:38,940 - experiments.base - INFO - 130/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 07:41:45,160 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:42:09,098 - experiments.base - INFO - reward_mean: -0.018715544857874782, reward_median: -0.017499999999999998, reward_std: 0.0057213976704523, reward_max: -0.010638297872340425, reward_min: -0.0325, runs: 2000
2019-04-08 07:42:09,114 - experiments.base - INFO - 131/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 07:42:15,249 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:42:39,581 - experiments.base - INFO - reward_mean: -0.019030645754580605, reward_median: -0.017499999999999998, reward_std: 0.00595569329104027, reward_max: -0.010857142857142857, reward_min: -0.0325, runs: 2000
2019-04-08 07:42:39,595 - experiments.base - INFO - 132/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 07:42:45,574 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:43:09,672 - experiments.base - INFO - reward_mean: -0.018705225212411348, reward_median: -0.016923076923076923, reward_std: 0.005726502999733509, reward_max: -0.010625, reward_min: -0.0325, runs: 2000
2019-04-08 07:43:09,687 - experiments.base - INFO - 133/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 07:44:02,037 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:44:41,494 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:44:41,502 - experiments.base - INFO - 134/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 07:45:28,925 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:46:08,172 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:46:08,181 - experiments.base - INFO - 135/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 07:46:52,547 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:47:31,835 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:47:31,843 - experiments.base - INFO - 136/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:47:57,919 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:48:25,611 - experiments.base - INFO - reward_mean: 0.030047730468679788, reward_median: 0.031708333333333324, reward_std: 0.025501633383882803, reward_max: 0.1290277777777778, reward_min: -0.011285714285714286, runs: 2000
2019-04-08 07:48:25,628 - experiments.base - INFO - 137/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:48:37,171 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:49:02,822 - experiments.base - INFO - reward_mean: -0.011298814337621583, reward_median: -0.011034482758620689, reward_std: 0.0009023187889139738, reward_max: -0.010180722891566265, reward_min: -0.017499999999999998, runs: 2000
2019-04-08 07:49:02,838 - experiments.base - INFO - 138/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:49:09,502 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:49:34,180 - experiments.base - INFO - reward_mean: -0.01682983787397442, reward_median: -0.015000000000000001, reward_std: 0.0053933790685955045, reward_max: -0.010599999999999998, reward_min: -0.0325, runs: 2000
2019-04-08 07:49:34,194 - experiments.base - INFO - 139/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:49:40,593 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:50:05,043 - experiments.base - INFO - reward_mean: -0.018891974301328367, reward_median: -0.017499999999999998, reward_std: 0.006034855877589331, reward_max: -0.010454545454545454, reward_min: -0.0325, runs: 2000
2019-04-08 07:50:05,059 - experiments.base - INFO - 140/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:50:11,487 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:50:36,146 - experiments.base - INFO - reward_mean: -0.01830658265500432, reward_median: -0.016428571428571428, reward_std: 0.005705730479233183, reward_max: -0.010608108108108107, reward_min: -0.0325, runs: 2000
2019-04-08 07:50:36,161 - experiments.base - INFO - 141/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 07:50:42,313 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:51:06,698 - experiments.base - INFO - reward_mean: -0.017581589917049164, reward_median: -0.015625, reward_std: 0.005642310312694833, reward_max: -0.010608108108108107, reward_min: -0.0325, runs: 2000
2019-04-08 07:51:06,712 - experiments.base - INFO - 142/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 07:51:12,892 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:51:37,252 - experiments.base - INFO - reward_mean: -0.01813281151421839, reward_median: -0.016428571428571428, reward_std: 0.005343308832050573, reward_max: -0.010573248407643312, reward_min: -0.0325, runs: 2000
2019-04-08 07:51:37,267 - experiments.base - INFO - 143/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 07:51:43,426 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:52:07,614 - experiments.base - INFO - reward_mean: -0.018730630684371235, reward_median: -0.017499999999999998, reward_std: 0.005737459622725807, reward_max: -0.010548780487804879, reward_min: -0.0325, runs: 2000
2019-04-08 07:52:07,628 - experiments.base - INFO - 144/216 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 07:52:13,835 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:52:38,100 - experiments.base - INFO - reward_mean: -0.018192915326889776, reward_median: -0.016428571428571428, reward_std: 0.005526426598295303, reward_max: -0.010671641791044777, reward_min: -0.0325, runs: 2000
2019-04-08 07:52:38,115 - experiments.base - INFO - 145/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 07:53:33,920 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:54:13,079 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:54:13,087 - experiments.base - INFO - 146/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 07:55:04,921 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:55:45,205 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:55:45,213 - experiments.base - INFO - 147/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 07:56:34,132 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:57:13,830 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 07:57:13,838 - experiments.base - INFO - 148/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 07:57:44,202 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:58:13,398 - experiments.base - INFO - reward_mean: 0.031813074892007284, reward_median: 0.028798449612403097, reward_std: 0.016059273811558205, reward_max: 0.1063953488372093, reward_min: -0.010633802816901409, runs: 2000
2019-04-08 07:58:13,415 - experiments.base - INFO - 149/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 07:58:20,918 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:58:45,603 - experiments.base - INFO - reward_mean: -0.01749533185513385, reward_median: -0.015625, reward_std: 0.005700060057435294, reward_max: -0.010299003322259135, reward_min: -0.0325, runs: 2000
2019-04-08 07:58:45,617 - experiments.base - INFO - 150/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 07:58:52,028 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:59:16,552 - experiments.base - INFO - reward_mean: -0.017925183622076162, reward_median: -0.016428571428571428, reward_std: 0.005611142748079956, reward_max: -0.010375, reward_min: -0.0325, runs: 2000
2019-04-08 07:59:16,568 - experiments.base - INFO - 151/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 07:59:22,840 - experiments.base - INFO - Took 2000 episodes
2019-04-08 07:59:47,500 - experiments.base - INFO - reward_mean: -0.018712405623544315, reward_median: -0.016923076923076923, reward_std: 0.005700352160035751, reward_max: -0.010559006211180123, reward_min: -0.0325, runs: 2000
2019-04-08 07:59:47,514 - experiments.base - INFO - 152/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 07:59:53,812 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:00:18,474 - experiments.base - INFO - reward_mean: -0.01826229990270365, reward_median: -0.016428571428571428, reward_std: 0.005369244173534601, reward_max: -0.010947368421052633, reward_min: -0.0325, runs: 2000
2019-04-08 08:00:18,489 - experiments.base - INFO - 153/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 08:00:24,843 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:00:50,401 - experiments.base - INFO - reward_mean: -0.018302705380742457, reward_median: -0.016428571428571428, reward_std: 0.0058280695604732945, reward_max: -0.010703125, reward_min: -0.0325, runs: 2000
2019-04-08 08:00:50,415 - experiments.base - INFO - 154/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 08:00:56,543 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:01:21,092 - experiments.base - INFO - reward_mean: -0.016725002841055256, reward_median: -0.01473684210526316, reward_std: 0.005526056216217522, reward_max: -0.01045, reward_min: -0.0325, runs: 2000
2019-04-08 08:01:21,107 - experiments.base - INFO - 155/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 08:01:27,347 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:01:51,872 - experiments.base - INFO - reward_mean: -0.018113119308844867, reward_median: -0.016428571428571428, reward_std: 0.005461857705955217, reward_max: -0.010731707317073172, reward_min: -0.0325, runs: 2000
2019-04-08 08:01:51,887 - experiments.base - INFO - 156/216 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 08:01:58,106 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:02:22,699 - experiments.base - INFO - reward_mean: -0.018268470657429987, reward_median: -0.016428571428571428, reward_std: 0.0056103018226861855, reward_max: -0.010633802816901409, reward_min: -0.0325, runs: 2000
2019-04-08 08:02:22,713 - experiments.base - INFO - 157/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:03:17,843 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:03:57,183 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:03:57,192 - experiments.base - INFO - 158/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 08:04:47,943 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:05:27,415 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:05:27,424 - experiments.base - INFO - 159/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 08:06:15,830 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:06:55,332 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:06:55,341 - experiments.base - INFO - 160/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:07:24,985 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:07:53,374 - experiments.base - INFO - reward_mean: 0.03297655357322185, reward_median: 0.0302008032128514, reward_std: 0.016157671242127532, reward_max: 0.12000000000000001, reward_min: 0.0015455594002306814, runs: 2000
2019-04-08 08:07:53,391 - experiments.base - INFO - 161/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:08:01,020 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:08:26,270 - experiments.base - INFO - reward_mean: -0.018437596421168583, reward_median: -0.016923076923076923, reward_std: 0.006002343146798883, reward_max: -0.010394736842105264, reward_min: -0.0325, runs: 2000
2019-04-08 08:08:26,284 - experiments.base - INFO - 162/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 08:08:32,696 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:08:57,126 - experiments.base - INFO - reward_mean: -0.0177410811931395, reward_median: -0.016428571428571428, reward_std: 0.00507001186943173, reward_max: -0.010526315789473684, reward_min: -0.0325, runs: 2000
2019-04-08 08:08:57,141 - experiments.base - INFO - 163/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 08:09:03,587 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:09:28,132 - experiments.base - INFO - reward_mean: -0.018096008403738456, reward_median: -0.016428571428571428, reward_std: 0.005418317547508499, reward_max: -0.010545454545454544, reward_min: -0.0325, runs: 2000
2019-04-08 08:09:28,148 - experiments.base - INFO - 164/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 08:09:34,474 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:09:59,037 - experiments.base - INFO - reward_mean: -0.018454968163723953, reward_median: -0.016428571428571428, reward_std: 0.006218813513213713, reward_max: -0.010393013100436681, reward_min: -0.0325, runs: 2000
2019-04-08 08:09:59,052 - experiments.base - INFO - 165/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 08:10:05,412 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:10:29,976 - experiments.base - INFO - reward_mean: -0.01848432224247378, reward_median: -0.016923076923076923, reward_std: 0.005720551585263422, reward_max: -0.01088235294117647, reward_min: -0.0325, runs: 2000
2019-04-08 08:10:29,991 - experiments.base - INFO - 166/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 08:10:36,330 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:11:00,880 - experiments.base - INFO - reward_mean: -0.01782624880690407, reward_median: -0.016, reward_std: 0.005511154084115715, reward_max: -0.010737704918032788, reward_min: -0.0325, runs: 2000
2019-04-08 08:11:00,895 - experiments.base - INFO - 167/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 08:11:07,607 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:11:32,070 - experiments.base - INFO - reward_mean: -0.01717754935167495, reward_median: -0.015625, reward_std: 0.0052751152146175255, reward_max: -0.010620689655172414, reward_min: -0.0325, runs: 2000
2019-04-08 08:11:32,084 - experiments.base - INFO - 168/216 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 08:11:38,415 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:12:03,063 - experiments.base - INFO - reward_mean: -0.015524385191241434, reward_median: -0.014090909090909093, reward_std: 0.004271601705618719, reward_max: -0.010476190476190476, reward_min: -0.0325, runs: 2000
2019-04-08 08:12:03,078 - experiments.base - INFO - 169/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:12:57,252 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:13:36,787 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:13:36,795 - experiments.base - INFO - 170/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 08:14:25,514 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:15:05,142 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:15:05,150 - experiments.base - INFO - 171/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 08:15:50,603 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:16:30,579 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:16:30,587 - experiments.base - INFO - 172/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:16:59,557 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:17:28,054 - experiments.base - INFO - reward_mean: 0.032805206438585255, reward_median: 0.030362903225806447, reward_std: 0.01657589450057482, reward_max: 0.133, reward_min: 0.0008922742110990219, runs: 2000
2019-04-08 08:17:28,071 - experiments.base - INFO - 173/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:17:35,661 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:18:00,473 - experiments.base - INFO - reward_mean: -0.018168646362206796, reward_median: -0.016428571428571428, reward_std: 0.006183676037016358, reward_max: -0.010224438902743141, reward_min: -0.0325, runs: 2000
2019-04-08 08:18:00,488 - experiments.base - INFO - 174/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 08:18:06,989 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:18:32,179 - experiments.base - INFO - reward_mean: -0.019003909444178995, reward_median: -0.017499999999999998, reward_std: 0.005950654159503397, reward_max: -0.010523255813953488, reward_min: -0.0325, runs: 2000
2019-04-08 08:18:32,195 - experiments.base - INFO - 175/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 08:18:38,626 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:19:03,274 - experiments.base - INFO - reward_mean: -0.015823244977089434, reward_median: -0.014090909090909093, reward_std: 0.004902181730684785, reward_max: -0.010466321243523313, reward_min: -0.0325, runs: 2000
2019-04-08 08:19:03,289 - experiments.base - INFO - 176/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 08:19:09,691 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:19:34,207 - experiments.base - INFO - reward_mean: -0.0188833435301448, reward_median: -0.017499999999999998, reward_std: 0.005832422022650334, reward_max: -0.010486486486486485, reward_min: -0.0325, runs: 2000
2019-04-08 08:19:34,223 - experiments.base - INFO - 177/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 08:19:40,621 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:20:05,337 - experiments.base - INFO - reward_mean: -0.01713682514424873, reward_median: -0.015625, reward_std: 0.005062071023243288, reward_max: -0.01079646017699115, reward_min: -0.0325, runs: 2000
2019-04-08 08:20:05,351 - experiments.base - INFO - 178/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 08:20:11,752 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:20:36,433 - experiments.base - INFO - reward_mean: -0.01555518610234161, reward_median: -0.014285714285714287, reward_std: 0.004073910636998262, reward_max: -0.010652173913043479, reward_min: -0.0325, runs: 2000
2019-04-08 08:20:36,449 - experiments.base - INFO - 179/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 08:20:42,813 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:21:07,068 - experiments.base - INFO - reward_mean: -0.01824298780303575, reward_median: -0.016, reward_std: 0.005909631504199914, reward_max: -0.010857142857142857, reward_min: -0.0325, runs: 2000
2019-04-08 08:21:07,082 - experiments.base - INFO - 180/216 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 08:21:13,447 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:21:38,484 - experiments.base - INFO - reward_mean: -0.0185151680456654, reward_median: -0.016923076923076923, reward_std: 0.005832054845029285, reward_max: -0.010687022900763359, reward_min: -0.0325, runs: 2000
2019-04-08 08:21:38,499 - experiments.base - INFO - 181/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:22:33,529 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:23:12,674 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:23:12,682 - experiments.base - INFO - 182/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 08:24:05,187 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:24:44,395 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:24:44,403 - experiments.base - INFO - 183/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 08:25:32,851 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:26:12,005 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:26:12,013 - experiments.base - INFO - 184/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:26:43,794 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:27:12,032 - experiments.base - INFO - reward_mean: 0.030939473595072684, reward_median: 0.028648648648648644, reward_std: 0.015157236247691497, reward_max: 0.10122222222222221, reward_min: -0.010391304347826086, runs: 2000
2019-04-08 08:27:12,051 - experiments.base - INFO - 185/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:27:28,630 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:27:55,680 - experiments.base - INFO - reward_mean: 0.0007866764525760752, reward_median: -0.010471204188481674, reward_std: 0.022360002262553556, reward_max: 0.10916666666666666, reward_min: -0.015625, runs: 2000
2019-04-08 08:27:55,698 - experiments.base - INFO - 186/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 08:28:02,088 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:28:26,752 - experiments.base - INFO - reward_mean: -0.0171478884922656, reward_median: -0.015294117647058824, reward_std: 0.005527614944631572, reward_max: -0.010483870967741934, reward_min: -0.0325, runs: 2000
2019-04-08 08:28:26,767 - experiments.base - INFO - 187/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 08:28:33,069 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:28:57,905 - experiments.base - INFO - reward_mean: -0.018934245269071916, reward_median: -0.017499999999999998, reward_std: 0.005896114175079786, reward_max: -0.010476190476190476, reward_min: -0.0325, runs: 2000
2019-04-08 08:28:57,920 - experiments.base - INFO - 188/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 08:29:04,144 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:29:28,449 - experiments.base - INFO - reward_mean: -0.01886101252460678, reward_median: -0.016923076923076923, reward_std: 0.0060722091123741094, reward_max: -0.010580645161290323, reward_min: -0.0325, runs: 2000
2019-04-08 08:29:28,464 - experiments.base - INFO - 189/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 08:29:34,669 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:29:59,041 - experiments.base - INFO - reward_mean: -0.01847407471334856, reward_median: -0.016428571428571428, reward_std: 0.005817228147773147, reward_max: -0.011, reward_min: -0.0325, runs: 2000
2019-04-08 08:29:59,055 - experiments.base - INFO - 190/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 08:30:05,337 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:30:29,476 - experiments.base - INFO - reward_mean: -0.01802074675102001, reward_median: -0.016, reward_std: 0.005884367346854241, reward_max: -0.010532544378698223, reward_min: -0.0325, runs: 2000
2019-04-08 08:30:29,490 - experiments.base - INFO - 191/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 08:30:35,637 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:30:59,781 - experiments.base - INFO - reward_mean: -0.015887945687277274, reward_median: -0.014090909090909093, reward_std: 0.004974916334356396, reward_max: -0.010529411764705881, reward_min: -0.0325, runs: 2000
2019-04-08 08:30:59,797 - experiments.base - INFO - 192/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 08:31:06,043 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:31:30,044 - experiments.base - INFO - reward_mean: -0.019002338383973465, reward_median: -0.017499999999999998, reward_std: 0.005970546871250011, reward_max: -0.010857142857142857, reward_min: -0.0325, runs: 2000
2019-04-08 08:31:30,059 - experiments.base - INFO - 193/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:32:25,414 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:33:04,236 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:33:04,245 - experiments.base - INFO - 194/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 08:33:54,497 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:34:33,899 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:34:33,908 - experiments.base - INFO - 195/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 08:35:20,883 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:36:00,067 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:36:00,075 - experiments.base - INFO - 196/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:36:30,444 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:36:59,679 - experiments.base - INFO - reward_mean: 0.031151284494922674, reward_median: 0.028648648648648644, reward_std: 0.018102795244871076, reward_max: 0.11670886075949367, reward_min: -0.01, runs: 2000
2019-04-08 08:36:59,694 - experiments.base - INFO - 197/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:37:16,259 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:37:43,369 - experiments.base - INFO - reward_mean: -0.002300730771666912, reward_median: -0.01045, reward_std: 0.018191587361401904, reward_max: 0.10247191011235957, reward_min: -0.016428571428571428, runs: 2000
2019-04-08 08:37:43,387 - experiments.base - INFO - 198/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 08:37:49,938 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:38:14,209 - experiments.base - INFO - reward_mean: -0.01891474421657545, reward_median: -0.017499999999999998, reward_std: 0.005753325150512982, reward_max: -0.010714285714285714, reward_min: -0.0325, runs: 2000
2019-04-08 08:38:14,224 - experiments.base - INFO - 199/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 08:38:20,612 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:38:44,690 - experiments.base - INFO - reward_mean: -0.01805159581361728, reward_median: -0.016, reward_std: 0.006091996344197794, reward_max: -0.010612244897959184, reward_min: -0.0325, runs: 2000
2019-04-08 08:38:44,703 - experiments.base - INFO - 200/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 08:38:51,124 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:39:15,897 - experiments.base - INFO - reward_mean: -0.018433532587323637, reward_median: -0.016428571428571428, reward_std: 0.005679690470009172, reward_max: -0.010703125, reward_min: -0.0325, runs: 2000
2019-04-08 08:39:15,912 - experiments.base - INFO - 201/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 08:39:22,261 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:39:46,795 - experiments.base - INFO - reward_mean: -0.01901902529694076, reward_median: -0.017499999999999998, reward_std: 0.005935455937419306, reward_max: -0.01081081081081081, reward_min: -0.0325, runs: 2000
2019-04-08 08:39:46,810 - experiments.base - INFO - 202/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 08:39:53,053 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:40:17,432 - experiments.base - INFO - reward_mean: -0.01851702390322953, reward_median: -0.016923076923076923, reward_std: 0.005673719756802146, reward_max: -0.010957446808510638, reward_min: -0.0325, runs: 2000
2019-04-08 08:40:17,447 - experiments.base - INFO - 203/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 08:40:23,715 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:40:48,262 - experiments.base - INFO - reward_mean: -0.017275636805316005, reward_median: -0.015000000000000001, reward_std: 0.005782160722752108, reward_max: -0.010714285714285714, reward_min: -0.0325, runs: 2000
2019-04-08 08:40:48,278 - experiments.base - INFO - 204/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 08:40:54,580 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:41:18,811 - experiments.base - INFO - reward_mean: -0.018900488070972585, reward_median: -0.017499999999999998, reward_std: 0.0057662177856480775, reward_max: -0.010737704918032788, reward_min: -0.0325, runs: 2000
2019-04-08 08:41:18,826 - experiments.base - INFO - 205/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:42:13,142 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:42:52,369 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:42:52,377 - experiments.base - INFO - 206/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.27
2019-04-08 08:43:40,346 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:44:19,905 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:44:19,914 - experiments.base - INFO - 207/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.53
2019-04-08 08:45:04,802 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:45:43,613 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:45:43,621 - experiments.base - INFO - 208/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:46:12,104 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:46:40,460 - experiments.base - INFO - reward_mean: 0.03469141451696589, reward_median: 0.03223628691983122, reward_std: 0.01782785081094373, reward_max: 0.1120731707317073, reward_min: -0.01088235294117647, runs: 2000
2019-04-08 08:46:40,477 - experiments.base - INFO - 209/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:46:55,921 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:47:22,846 - experiments.base - INFO - reward_mean: -0.0013512528305112536, reward_median: -0.010569620253164557, reward_std: 0.02156887940571054, reward_max: 0.1290277777777778, reward_min: -0.019, runs: 2000
2019-04-08 08:47:22,863 - experiments.base - INFO - 210/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.913
2019-04-08 08:47:29,781 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:47:54,213 - experiments.base - INFO - reward_mean: -0.01855968466998412, reward_median: -0.016923076923076923, reward_std: 0.0057175357425417244, reward_max: -0.01062937062937063, reward_min: -0.0325, runs: 2000
2019-04-08 08:47:54,228 - experiments.base - INFO - 211/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.926
2019-04-08 08:48:00,686 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:48:25,437 - experiments.base - INFO - reward_mean: -0.016617501185584138, reward_median: -0.01473684210526316, reward_std: 0.005121360648382505, reward_max: -0.010468749999999999, reward_min: -0.0325, runs: 2000
2019-04-08 08:48:25,453 - experiments.base - INFO - 212/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.939
2019-04-08 08:48:31,977 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:48:56,057 - experiments.base - INFO - reward_mean: -0.01834824138048301, reward_median: -0.016428571428571428, reward_std: 0.005834152636637993, reward_max: -0.011046511627906977, reward_min: -0.0325, runs: 2000
2019-04-08 08:48:56,072 - experiments.base - INFO - 213/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.951
2019-04-08 08:49:02,462 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:49:27,353 - experiments.base - INFO - reward_mean: -0.0179348559022004, reward_median: -0.016, reward_std: 0.005897080047951409, reward_max: -0.010737704918032788, reward_min: -0.0325, runs: 2000
2019-04-08 08:49:27,367 - experiments.base - INFO - 214/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.964
2019-04-08 08:49:33,710 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:49:57,875 - experiments.base - INFO - reward_mean: -0.01747299462699965, reward_median: -0.015000000000000001, reward_std: 0.005959850528658047, reward_max: -0.010687022900763359, reward_min: -0.0325, runs: 2000
2019-04-08 08:49:57,890 - experiments.base - INFO - 215/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.977
2019-04-08 08:50:04,243 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:50:29,809 - experiments.base - INFO - reward_mean: -0.012470409499819146, reward_median: -0.011800000000000001, reward_std: 0.0022066095679253222, reward_max: -0.010173745173745172, reward_min: -0.025000000000000005, runs: 2000
2019-04-08 08:50:29,826 - experiments.base - INFO - 216/216 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.99
2019-04-08 08:50:36,092 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:51:00,522 - experiments.base - INFO - reward_mean: -0.01683745509015212, reward_median: -0.015000000000000001, reward_std: 0.00533283577605469, reward_max: -0.010511363636363636, reward_min: -0.0325, runs: 2000
2019-04-08 08:51:00,541 - __main__ - INFO - Running Q experiment: Frozen Lake (8x8)
2019-04-08 08:51:00,542 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-08 08:51:00,542 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:51:43,941 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:52:02,418 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:52:02,427 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 08:52:51,048 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:53:09,484 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:53:09,492 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 08:53:21,660 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:53:27,218 - experiments.base - INFO - reward_mean: -0.004996144256150215, reward_median: -0.010545454545454544, reward_std: 0.009275273915695003, reward_max: 0.04941176470588235, reward_min: -0.016, runs: 2000
2019-04-08 08:53:27,235 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 08:53:42,673 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:53:49,278 - experiments.base - INFO - reward_mean: -0.004408096382220566, reward_median: -0.006286764705882352, reward_std: 0.007776260540960565, reward_max: 0.03809523809523809, reward_min: -0.014285714285714287, runs: 2000
2019-04-08 08:53:49,295 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 08:53:59,566 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:54:05,154 - experiments.base - INFO - reward_mean: -0.004897285014208125, reward_median: -0.010517241379310345, reward_std: 0.008993613974449238, reward_max: 0.04611111111111111, reward_min: -0.015294117647058824, runs: 2000
2019-04-08 08:54:05,171 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 08:54:16,593 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:54:22,169 - experiments.base - INFO - reward_mean: 0.00010054388877202819, reward_median: 1.09923071745065e-18, reward_std: 0.009236457796612562, reward_max: 0.0431578947368421, reward_min: -0.015625, runs: 2000
2019-04-08 08:54:22,186 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 08:54:32,598 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:54:37,839 - experiments.base - INFO - reward_mean: -0.0004856205665303135, reward_median: -0.0004716981132075455, reward_std: 0.010374263164889696, reward_max: 0.05733333333333333, reward_min: -0.015625, runs: 2000
2019-04-08 08:54:37,857 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 08:54:47,752 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:54:53,059 - experiments.base - INFO - reward_mean: -0.0016732185243202354, reward_median: -0.002377249943039417, reward_std: 0.0100520932631413, reward_max: 0.0431578947368421, reward_min: -0.015000000000000001, runs: 2000
2019-04-08 08:54:53,078 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:55:01,692 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:55:07,174 - experiments.base - INFO - reward_mean: -0.005996226706122009, reward_median: -0.010789473684210526, reward_std: 0.008882488288219746, reward_max: 0.0431578947368421, reward_min: -0.015000000000000001, runs: 2000
2019-04-08 08:55:07,190 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:55:12,523 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:55:17,284 - experiments.base - INFO - reward_mean: -0.006710806115810716, reward_median: -0.011666666666666667, reward_std: 0.011025161424110955, reward_max: 0.04611111111111111, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 08:55:17,300 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 08:56:00,210 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:56:18,905 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:56:18,913 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 08:57:04,480 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:57:23,131 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 08:57:23,139 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 08:57:36,628 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:57:43,140 - experiments.base - INFO - reward_mean: -0.007027769707717759, reward_median: -0.010445544554455444, reward_std: 0.006869978106695891, reward_max: 0.033913043478260865, reward_min: -0.015294117647058824, runs: 2000
2019-04-08 08:57:43,158 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 08:57:57,067 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:58:02,936 - experiments.base - INFO - reward_mean: -0.003349053027834792, reward_median: -0.004598930481283421, reward_std: 0.008361892285752853, reward_max: 0.035909090909090904, reward_min: -0.01375, runs: 2000
2019-04-08 08:58:02,953 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 08:58:15,432 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:58:20,887 - experiments.base - INFO - reward_mean: -0.005166904660559088, reward_median: -0.010610176503033645, reward_std: 0.00845789099220837, reward_max: 0.03809523809523809, reward_min: -0.013913043478260872, runs: 2000
2019-04-08 08:58:20,904 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 08:58:32,218 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:58:37,757 - experiments.base - INFO - reward_mean: -0.004310694181221315, reward_median: -0.010381362779423647, reward_std: 0.00897086474086292, reward_max: 0.04941176470588235, reward_min: -0.015000000000000001, runs: 2000
2019-04-08 08:58:37,774 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 08:58:47,715 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:58:53,278 - experiments.base - INFO - reward_mean: 0.0021175624304115162, reward_median: 0.0010989010989011, reward_std: 0.008384190671586524, reward_max: 0.04941176470588235, reward_min: -0.014285714285714287, runs: 2000
2019-04-08 08:58:53,297 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 08:59:02,632 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:59:07,929 - experiments.base - INFO - reward_mean: -0.005734142418455551, reward_median: -0.01088235294117647, reward_std: 0.009735336754506128, reward_max: 0.04611111111111111, reward_min: -0.01473684210526316, runs: 2000
2019-04-08 08:59:07,948 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 08:59:16,761 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:59:22,841 - experiments.base - INFO - reward_mean: -0.0012143563137238115, reward_median: -0.001984126984126984, reward_std: 0.008875153740557412, reward_max: 0.0431578947368421, reward_min: -0.016923076923076923, runs: 2000
2019-04-08 08:59:22,859 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 08:59:28,641 - experiments.base - INFO - Took 2000 episodes
2019-04-08 08:59:33,647 - experiments.base - INFO - reward_mean: -0.0005900816798827544, reward_median: -0.0011788710907704035, reward_std: 0.01190522571926433, reward_max: 0.05733333333333333, reward_min: -0.019, runs: 2000
2019-04-08 08:59:33,667 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:00:15,322 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:00:34,060 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:00:34,069 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:01:19,339 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:01:37,844 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:01:37,853 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:01:51,693 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:01:57,386 - experiments.base - INFO - reward_mean: -0.00529282669070067, reward_median: -0.010504205636808737, reward_std: 0.008540724668645408, reward_max: 0.03809523809523809, reward_min: -0.015625, runs: 2000
2019-04-08 09:01:57,425 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:02:11,653 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:02:17,695 - experiments.base - INFO - reward_mean: -0.00016552195953079972, reward_median: -0.0014406779661016943, reward_std: 0.007333255459699226, reward_max: 0.040499999999999994, reward_min: -0.013000000000000001, runs: 2000
2019-04-08 09:02:17,714 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:02:29,417 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:02:34,713 - experiments.base - INFO - reward_mean: -0.0040530533583892385, reward_median: -0.010669154228855723, reward_std: 0.009664579633195607, reward_max: 0.0431578947368421, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 09:02:34,731 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:02:45,624 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:02:51,093 - experiments.base - INFO - reward_mean: -0.005140324195319674, reward_median: -0.010562499999999999, reward_std: 0.00862531349257638, reward_max: 0.040499999999999994, reward_min: -0.015000000000000001, runs: 2000
2019-04-08 09:02:51,110 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:03:02,190 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:03:07,765 - experiments.base - INFO - reward_mean: -0.0022507768184721363, reward_median: -0.0037267080745341605, reward_std: 0.009904052638512172, reward_max: 0.03809523809523809, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 09:03:07,781 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:03:16,227 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:03:21,309 - experiments.base - INFO - reward_mean: -0.003222960724354709, reward_median: -0.010789473684210526, reward_std: 0.01082310357895608, reward_max: 0.0431578947368421, reward_min: -0.016428571428571428, runs: 2000
2019-04-08 09:03:21,327 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:03:29,658 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:03:34,952 - experiments.base - INFO - reward_mean: 0.0016462836458562124, reward_median: 0.001744186046511628, reward_std: 0.00965086822957557, reward_max: 0.0431578947368421, reward_min: -0.017499999999999998, runs: 2000
2019-04-08 09:03:34,969 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:03:40,503 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:03:45,337 - experiments.base - INFO - reward_mean: -0.006832573990677873, reward_median: -0.01152542372881356, reward_std: 0.011137787749032815, reward_max: 0.053125, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:03:45,353 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:04:29,679 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:04:48,570 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:04:48,578 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:05:37,264 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:05:56,089 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:05:56,097 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:06:08,520 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:06:14,294 - experiments.base - INFO - reward_mean: -0.005824115110139175, reward_median: -0.010548780487804879, reward_std: 0.00843535496376387, reward_max: 0.035909090909090904, reward_min: -0.01473684210526316, runs: 2000
2019-04-08 09:06:14,311 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:06:28,405 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:06:34,670 - experiments.base - INFO - reward_mean: -0.003921605420295995, reward_median: -0.005521042281219271, reward_std: 0.007967877140071161, reward_max: 0.03809523809523809, reward_min: -0.016428571428571428, runs: 2000
2019-04-08 09:06:34,688 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:06:46,541 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:06:52,228 - experiments.base - INFO - reward_mean: -0.0023098302789429682, reward_median: -0.0032666666666666664, reward_std: 0.008775383781957947, reward_max: 0.03809523809523809, reward_min: -0.013913043478260872, runs: 2000
2019-04-08 09:06:52,246 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:07:03,132 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:07:08,627 - experiments.base - INFO - reward_mean: -0.005700818890311763, reward_median: -0.010803571428571428, reward_std: 0.009114452666033198, reward_max: 0.03809523809523809, reward_min: -0.01473684210526316, runs: 2000
2019-04-08 09:07:08,645 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:07:18,736 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:07:24,159 - experiments.base - INFO - reward_mean: 0.0015500416129457131, reward_median: 0.0013483146067415743, reward_std: 0.008864895140012015, reward_max: 0.033913043478260865, reward_min: -0.014285714285714287, runs: 2000
2019-04-08 09:07:24,177 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:07:34,648 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:07:40,371 - experiments.base - INFO - reward_mean: -0.0023343210237384265, reward_median: -0.0036477987421383645, reward_std: 0.009341453309910394, reward_max: 0.040499999999999994, reward_min: -0.015625, runs: 2000
2019-04-08 09:07:40,387 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:07:49,387 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:07:54,713 - experiments.base - INFO - reward_mean: 0.002068416724814173, reward_median: 0.001744186046511628, reward_std: 0.009313965962385946, reward_max: 0.040499999999999994, reward_min: -0.016, runs: 2000
2019-04-08 09:07:54,729 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:08:00,847 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:08:05,925 - experiments.base - INFO - reward_mean: 0.0001340173410895179, reward_median: 0.000688129899216126, reward_std: 0.011948194113017074, reward_max: 0.053125, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:08:05,944 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:08:49,890 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:09:08,629 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:09:08,637 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:09:56,272 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:10:14,967 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:10:14,975 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:10:29,323 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:10:34,859 - experiments.base - INFO - reward_mean: -0.005705141179568673, reward_median: -0.010671641791044777, reward_std: 0.008869387840200115, reward_max: 0.04611111111111111, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 09:10:34,876 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:10:46,469 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:10:51,942 - experiments.base - INFO - reward_mean: -0.0016937652368806645, reward_median: -0.002170542635658915, reward_std: 0.009748438403344207, reward_max: 0.0431578947368421, reward_min: -0.01375, runs: 2000
2019-04-08 09:10:51,958 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:11:04,507 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:11:10,011 - experiments.base - INFO - reward_mean: 0.0003494164090407702, reward_median: -9.803921568627242e-05, reward_std: 0.009075265232468693, reward_max: 0.0431578947368421, reward_min: -0.01375, runs: 2000
2019-04-08 09:11:10,028 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:11:20,483 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:11:25,855 - experiments.base - INFO - reward_mean: -0.004762934301256326, reward_median: -0.010652173913043479, reward_std: 0.008957775442388799, reward_max: 0.035909090909090904, reward_min: -0.01473684210526316, runs: 2000
2019-04-08 09:11:25,872 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:11:35,659 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:11:40,903 - experiments.base - INFO - reward_mean: -0.004359080284363608, reward_median: -0.010818181818181817, reward_std: 0.01004426656541969, reward_max: 0.040499999999999994, reward_min: -0.015294117647058824, runs: 2000
2019-04-08 09:11:40,920 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:11:50,047 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:11:55,383 - experiments.base - INFO - reward_mean: -0.005427059846496167, reward_median: -0.010841121495327103, reward_std: 0.009825730273791917, reward_max: 0.0431578947368421, reward_min: -0.01473684210526316, runs: 2000
2019-04-08 09:11:55,402 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:12:03,459 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:12:08,737 - experiments.base - INFO - reward_mean: 0.0017596539539428014, reward_median: 0.0018132694938440496, reward_std: 0.010102089720998091, reward_max: 0.04941176470588235, reward_min: -0.016, runs: 2000
2019-04-08 09:12:08,753 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:12:14,668 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:12:19,909 - experiments.base - INFO - reward_mean: -0.0018372342350818564, reward_median: -0.0026811594202898552, reward_std: 0.011392709393516377, reward_max: 0.0431578947368421, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:12:19,928 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:13:00,827 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:13:19,671 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:13:19,680 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:14:05,796 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:14:24,706 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:14:24,714 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:14:37,293 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:14:42,924 - experiments.base - INFO - reward_mean: -0.0020503063338808175, reward_median: -0.003288962472406181, reward_std: 0.009344949681543735, reward_max: 0.03809523809523809, reward_min: -0.0136, runs: 2000
2019-04-08 09:14:42,941 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:14:55,221 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:15:01,125 - experiments.base - INFO - reward_mean: -0.0019164486815915224, reward_median: -0.0029370629370629375, reward_std: 0.008908759792603234, reward_max: 0.04941176470588235, reward_min: -0.014285714285714287, runs: 2000
2019-04-08 09:15:01,143 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:15:12,127 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:15:17,567 - experiments.base - INFO - reward_mean: -0.00526209901833807, reward_median: -0.01076271186440678, reward_std: 0.009736674270746945, reward_max: 0.04941176470588235, reward_min: -0.015294117647058824, runs: 2000
2019-04-08 09:15:17,585 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:15:27,169 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:15:32,408 - experiments.base - INFO - reward_mean: -0.004616120979857818, reward_median: -0.010923101199242582, reward_std: 0.010178529326640338, reward_max: 0.03809523809523809, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 09:15:32,426 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:15:42,248 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:15:47,748 - experiments.base - INFO - reward_mean: -0.0005298734925220685, reward_median: -0.0006481481481481467, reward_std: 0.01003382763782404, reward_max: 0.040499999999999994, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 09:15:47,767 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:15:57,888 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:16:03,461 - experiments.base - INFO - reward_mean: 0.0021699940288975694, reward_median: 0.0010989010989011, reward_std: 0.008356251119069567, reward_max: 0.040499999999999994, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 09:16:03,477 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:16:11,512 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:16:17,124 - experiments.base - INFO - reward_mean: -0.0015691217468562395, reward_median: -0.002230769230769231, reward_std: 0.009898085939058441, reward_max: 0.0431578947368421, reward_min: -0.016, runs: 2000
2019-04-08 09:16:17,141 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:16:22,598 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:16:27,707 - experiments.base - INFO - reward_mean: -0.0018674893827890255, reward_median: -0.0033769779841761258, reward_std: 0.011252097870501795, reward_max: 0.04941176470588235, reward_min: -0.02, runs: 2000
2019-04-08 09:16:27,726 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:17:18,907 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:17:37,791 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:17:37,799 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:18:29,495 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:18:48,975 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:18:48,983 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:19:37,500 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:19:56,709 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:19:56,717 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:20:15,774 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:20:22,398 - experiments.base - INFO - reward_mean: -0.0008046986612277506, reward_median: -0.0024060150375939853, reward_std: 0.006245211647379529, reward_max: 0.0431578947368421, reward_min: -0.008935721812434141, runs: 2000
2019-04-08 09:20:22,417 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:20:40,774 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:20:47,909 - experiments.base - INFO - reward_mean: -0.0017152091853221462, reward_median: -0.003483870967741935, reward_std: 0.006146407051384086, reward_max: 0.03809523809523809, reward_min: -0.01, runs: 2000
2019-04-08 09:20:47,926 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:21:07,392 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:21:15,788 - experiments.base - INFO - reward_mean: -0.0035986882448377157, reward_median: -0.005450450450450449, reward_std: 0.005890052428925513, reward_max: 0.040499999999999994, reward_min: -0.01, runs: 2000
2019-04-08 09:21:15,803 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:21:27,488 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:21:33,211 - experiments.base - INFO - reward_mean: 0.0006386967341062014, reward_median: -0.00028846153846153713, reward_std: 0.00907961027070291, reward_max: 0.04611111111111111, reward_min: -0.01473684210526316, runs: 2000
2019-04-08 09:21:33,229 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:21:44,980 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:21:50,541 - experiments.base - INFO - reward_mean: 0.0018212871605611275, reward_median: 0.0006315789473684216, reward_std: 0.007974117854411343, reward_max: 0.033913043478260865, reward_min: -0.01375, runs: 2000
2019-04-08 09:21:50,559 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:22:03,270 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:22:09,525 - experiments.base - INFO - reward_mean: -0.0035858922349052144, reward_median: -0.005108930162750339, reward_std: 0.008131684973312483, reward_max: 0.03809523809523809, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 09:22:09,542 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:22:16,773 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:22:21,944 - experiments.base - INFO - reward_mean: -0.0006961053153558794, reward_median: -0.0009415218790218781, reward_std: 0.01198804698788664, reward_max: 0.053125, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:22:21,963 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:23:10,894 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:23:29,912 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:23:29,920 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:24:20,363 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:24:39,378 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:24:39,387 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:25:25,783 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:25:44,651 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:25:44,659 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:26:03,338 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:26:10,083 - experiments.base - INFO - reward_mean: -0.0007758466523863052, reward_median: -0.0026544483232836136, reward_std: 0.006391990236150248, reward_max: 0.04611111111111111, reward_min: -0.01, runs: 2000
2019-04-08 09:26:10,100 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:26:27,877 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:26:35,613 - experiments.base - INFO - reward_mean: -0.0028424659248203745, reward_median: -0.004684210526315788, reward_std: 0.005943626444089398, reward_max: 0.0431578947368421, reward_min: -0.01, runs: 2000
2019-04-08 09:26:35,629 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:26:52,163 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:26:59,291 - experiments.base - INFO - reward_mean: -0.002674987271502807, reward_median: -0.0043258426966292125, reward_std: 0.00624898486379431, reward_max: 0.03208333333333333, reward_min: -0.012812500000000001, runs: 2000
2019-04-08 09:26:59,308 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:27:14,161 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:27:20,803 - experiments.base - INFO - reward_mean: -0.00020778384890188698, reward_median: -0.001984126984126984, reward_std: 0.0065997381906348525, reward_max: 0.04611111111111111, reward_min: -0.01, runs: 2000
2019-04-08 09:27:20,821 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:27:32,786 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:27:38,636 - experiments.base - INFO - reward_mean: -0.0007221184781646645, reward_median: -0.0015833333333333333, reward_std: 0.008250557060197337, reward_max: 0.053125, reward_min: -0.014500000000000002, runs: 2000
2019-04-08 09:27:38,665 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:27:50,385 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:27:56,358 - experiments.base - INFO - reward_mean: -0.0013388356255688767, reward_median: -0.001984126984126984, reward_std: 0.007569248202452714, reward_max: 0.04941176470588235, reward_min: -0.01310344827586207, runs: 2000
2019-04-08 09:27:56,377 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:28:04,466 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:28:09,741 - experiments.base - INFO - reward_mean: 0.0010348280485716127, reward_median: 0.0010989010989011, reward_std: 0.010388694192756188, reward_max: 0.04941176470588235, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:28:09,759 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:28:57,734 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:29:16,596 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:29:16,604 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:30:04,131 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:30:23,171 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:30:23,179 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:31:05,754 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:31:24,298 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:31:24,306 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:31:42,292 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:31:48,552 - experiments.base - INFO - reward_mean: 0.00044577633099477634, reward_median: -0.0011403508771929815, reward_std: 0.006574311652331453, reward_max: 0.035909090909090904, reward_min: -0.008508124076809452, runs: 2000
2019-04-08 09:31:48,570 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:32:04,799 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:32:12,209 - experiments.base - INFO - reward_mean: -0.0025117810029804902, reward_median: -0.0043575418994413395, reward_std: 0.005977737445453387, reward_max: 0.03809523809523809, reward_min: -0.01, runs: 2000
2019-04-08 09:32:12,225 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:32:29,078 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:32:35,605 - experiments.base - INFO - reward_mean: -0.0013397171394150505, reward_median: -0.00298611111111111, reward_std: 0.006932787533339487, reward_max: 0.035909090909090904, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 09:32:35,621 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:32:49,025 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:32:55,001 - experiments.base - INFO - reward_mean: -0.0010135162803606755, reward_median: -0.0019199999999999998, reward_std: 0.0075841424126134455, reward_max: 0.03208333333333333, reward_min: -0.01375, runs: 2000
2019-04-08 09:32:55,017 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:33:06,198 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:33:11,730 - experiments.base - INFO - reward_mean: 0.0006940862976833494, reward_median: 1.09923071745065e-18, reward_std: 0.008803284256888295, reward_max: 0.04941176470588235, reward_min: -0.013913043478260872, runs: 2000
2019-04-08 09:33:11,748 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:33:21,786 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:33:27,118 - experiments.base - INFO - reward_mean: -0.0011891621316597392, reward_median: -0.0014406779661016943, reward_std: 0.00988906117526475, reward_max: 0.04611111111111111, reward_min: -0.017499999999999998, runs: 2000
2019-04-08 09:33:27,134 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:33:34,490 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:33:39,543 - experiments.base - INFO - reward_mean: -0.0031594161186829276, reward_median: -0.010452364113962415, reward_std: 0.010637216730891595, reward_max: 0.04611111111111111, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:33:39,561 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:34:31,872 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:34:50,546 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:34:50,554 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:35:42,095 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:36:00,740 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:36:00,748 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:36:49,219 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:37:07,838 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:37:07,847 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:37:53,935 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:38:12,549 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:38:12,557 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:38:31,611 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:38:38,569 - experiments.base - INFO - reward_mean: -0.0016641425869640604, reward_median: -0.003441558441558441, reward_std: 0.0062654070367764245, reward_max: 0.0431578947368421, reward_min: -0.01, runs: 2000
2019-04-08 09:38:38,585 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:38:58,909 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:39:05,506 - experiments.base - INFO - reward_mean: -0.0007330579284306553, reward_median: -0.002348484848484849, reward_std: 0.006481362438444518, reward_max: 0.0431578947368421, reward_min: -0.01, runs: 2000
2019-04-08 09:39:05,523 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:39:19,700 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:39:26,150 - experiments.base - INFO - reward_mean: 0.00041328875563468437, reward_median: -0.0006481481481481467, reward_std: 0.007122099470261898, reward_max: 0.0431578947368421, reward_min: -0.013000000000000001, runs: 2000
2019-04-08 09:39:26,167 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:39:38,448 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:39:43,954 - experiments.base - INFO - reward_mean: 0.002198655257702436, reward_median: 0.0012222222222222233, reward_std: 0.008096896000880533, reward_max: 0.040499999999999994, reward_min: -0.01375, runs: 2000
2019-04-08 09:39:43,971 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:39:56,044 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:40:01,840 - experiments.base - INFO - reward_mean: -0.0022719243711491403, reward_median: -0.002759763617677287, reward_std: 0.008517564606042086, reward_max: 0.0431578947368421, reward_min: -0.014285714285714287, runs: 2000
2019-04-08 09:40:01,857 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:40:09,447 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:40:14,671 - experiments.base - INFO - reward_mean: -0.0030207806016759233, reward_median: -0.004974626828724904, reward_std: 0.01079860003525186, reward_max: 0.053125, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:40:14,689 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:41:04,444 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:41:23,056 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:41:23,065 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:42:12,093 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:42:30,765 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:42:30,774 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:43:17,857 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:43:36,434 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:43:36,442 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:44:18,424 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:44:37,625 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:44:37,633 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:44:55,850 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:45:02,355 - experiments.base - INFO - reward_mean: -0.0005043829249688213, reward_median: -0.002170542635658915, reward_std: 0.006593796418032962, reward_max: 0.03809523809523809, reward_min: -0.01, runs: 2000
2019-04-08 09:45:02,372 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:45:20,665 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:45:27,418 - experiments.base - INFO - reward_mean: -0.0012168499089256718, reward_median: -0.0033112582781456954, reward_std: 0.006480183550668073, reward_max: 0.04611111111111111, reward_min: -0.00887902330743618, runs: 2000
2019-04-08 09:45:27,435 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:45:40,769 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:45:46,859 - experiments.base - INFO - reward_mean: 9.59738821908305e-05, reward_median: -0.0012931034482758614, reward_std: 0.007789045998672837, reward_max: 0.0431578947368421, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 09:45:46,877 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:46:01,218 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:46:07,108 - experiments.base - INFO - reward_mean: 0.0002625072777312422, reward_median: -0.0010619469026548662, reward_std: 0.007008965171786752, reward_max: 0.03809523809523809, reward_min: -0.013333333333333332, runs: 2000
2019-04-08 09:46:07,125 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:46:17,877 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:46:23,468 - experiments.base - INFO - reward_mean: -0.00022359168213198166, reward_median: -0.0009009009009008997, reward_std: 0.008369804375374768, reward_max: 0.0431578947368421, reward_min: -0.0136, runs: 2000
2019-04-08 09:46:23,486 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:46:31,321 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:46:36,359 - experiments.base - INFO - reward_mean: 0.0001863538534829802, reward_median: 0.00041237113402062004, reward_std: 0.011744742203318115, reward_max: 0.04941176470588235, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:46:36,380 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:47:23,918 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:47:42,540 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:47:42,548 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:48:30,483 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:48:49,302 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:48:49,311 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:49:33,333 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:49:52,551 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:49:52,559 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:50:24,715 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:50:44,048 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:50:44,056 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:51:01,298 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:51:07,386 - experiments.base - INFO - reward_mean: 0.00020490904029806117, reward_median: -0.0015126050420168062, reward_std: 0.006478111831568694, reward_max: 0.04941176470588235, reward_min: -0.00834697217675941, runs: 2000
2019-04-08 09:51:07,405 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:51:22,986 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:51:29,548 - experiments.base - INFO - reward_mean: -0.0008299088269530315, reward_median: -0.002627737226277372, reward_std: 0.006360678799758649, reward_max: 0.035909090909090904, reward_min: -0.01, runs: 2000
2019-04-08 09:51:29,564 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:51:44,056 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:51:50,554 - experiments.base - INFO - reward_mean: -0.001114924730405931, reward_median: -0.002573529411764705, reward_std: 0.006602988209112499, reward_max: 0.04611111111111111, reward_min: -0.012647058823529414, runs: 2000
2019-04-08 09:51:50,571 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 09:52:01,529 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:52:07,364 - experiments.base - INFO - reward_mean: -0.0013989491005529583, reward_median: -0.0024060150375939853, reward_std: 0.00827897060420464, reward_max: 0.04941176470588235, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 09:52:07,382 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 09:52:18,354 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:52:24,183 - experiments.base - INFO - reward_mean: -0.0007242784168422942, reward_median: -0.0014406779661016943, reward_std: 0.007890733709950784, reward_max: 0.0431578947368421, reward_min: -0.013333333333333332, runs: 2000
2019-04-08 09:52:24,200 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 09:52:32,167 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:52:37,484 - experiments.base - INFO - reward_mean: -0.0033797782162655545, reward_median: -0.0047258126090750415, reward_std: 0.01015407758448326, reward_max: 0.040499999999999994, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 09:52:37,504 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 09:53:29,867 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:53:48,766 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:53:48,775 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 09:54:40,329 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:54:59,575 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:54:59,584 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 09:55:51,788 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:56:10,400 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:56:10,409 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 09:57:01,092 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:57:19,849 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:57:19,857 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 09:58:09,570 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:58:28,247 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:58:28,254 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 09:59:14,816 - experiments.base - INFO - Took 2000 episodes
2019-04-08 09:59:33,687 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 09:59:33,695 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 09:59:53,581 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:00:00,755 - experiments.base - INFO - reward_mean: -0.0016117965650627277, reward_median: -0.0035668789808917193, reward_std: 0.0064273286338730655, reward_max: 0.040499999999999994, reward_min: -0.01, runs: 2000
2019-04-08 10:00:00,770 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:00:20,445 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:00:27,120 - experiments.base - INFO - reward_mean: -0.0006534715245158793, reward_median: -0.0024060150375939853, reward_std: 0.006563414451274468, reward_max: 0.04611111111111111, reward_min: -0.008930084745762712, runs: 2000
2019-04-08 10:00:27,139 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:00:44,242 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:00:50,691 - experiments.base - INFO - reward_mean: -8.962056756257624e-05, reward_median: -0.0017213114754098357, reward_std: 0.006450119210736254, reward_max: 0.053125, reward_min: -0.008857466063348415, runs: 2000
2019-04-08 10:00:50,709 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:00:55,567 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:01:00,194 - experiments.base - INFO - reward_mean: -0.012940185383624634, reward_median: -0.016, reward_std: 0.010948287099968267, reward_max: 0.04941176470588235, reward_min: -0.028000000000000004, runs: 2000
2019-04-08 10:01:00,209 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:01:50,695 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:02:09,308 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:02:09,316 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:02:59,205 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:03:17,900 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:03:17,908 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:04:07,121 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:04:25,868 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:04:25,877 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:05:14,945 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:05:34,209 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:05:34,217 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:06:23,732 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:06:42,411 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:06:42,420 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:07:30,150 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:07:48,792 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:07:48,800 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:08:08,456 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:08:15,217 - experiments.base - INFO - reward_mean: -0.0011544002581857099, reward_median: -0.0028112968591691998, reward_std: 0.00616331184957503, reward_max: 0.03809523809523809, reward_min: -0.01, runs: 2000
2019-04-08 10:08:15,234 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:08:32,125 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:08:38,151 - experiments.base - INFO - reward_mean: 0.00033086418973038125, reward_median: -0.0012931034482758614, reward_std: 0.006213150989513097, reward_max: 0.04611111111111111, reward_min: -0.00829103214890017, runs: 2000
2019-04-08 10:08:38,170 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:08:54,605 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:09:02,231 - experiments.base - INFO - reward_mean: -0.002578232425952134, reward_median: -0.00451086956521739, reward_std: 0.005913075233180254, reward_max: 0.03208333333333333, reward_min: -0.01, runs: 2000
2019-04-08 10:09:02,247 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:09:06,469 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:09:10,844 - experiments.base - INFO - reward_mean: -0.01649677472305493, reward_median: -0.016428571428571428, reward_std: 0.006914794810549206, reward_max: 0.04941176470588235, reward_min: -0.028000000000000004, runs: 2000
2019-04-08 10:09:10,861 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:09:59,957 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:10:18,606 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:10:18,614 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:11:06,864 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:11:25,918 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:11:25,926 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:12:13,081 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:12:31,996 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:12:32,005 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:13:18,974 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:13:38,172 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:13:38,180 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:14:24,716 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:14:43,655 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:14:43,663 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:15:26,057 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:15:45,444 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:15:45,453 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:16:04,126 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:16:11,070 - experiments.base - INFO - reward_mean: -0.001364922451055744, reward_median: -0.003034482758620689, reward_std: 0.006123234762300072, reward_max: 0.033913043478260865, reward_min: -0.01, runs: 2000
2019-04-08 10:16:11,087 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:16:27,401 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:16:35,060 - experiments.base - INFO - reward_mean: -0.002583092309751912, reward_median: -0.0043575418994413395, reward_std: 0.005679098346770482, reward_max: 0.0304, reward_min: -0.01, runs: 2000
2019-04-08 10:16:35,075 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:16:48,670 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:16:54,985 - experiments.base - INFO - reward_mean: 0.00023220742967867714, reward_median: -0.0014766415040592504, reward_std: 0.006477834273299238, reward_max: 0.03809523809523809, reward_min: -0.008740648379052366, runs: 2000
2019-04-08 10:16:55,004 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:16:59,494 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:17:04,231 - experiments.base - INFO - reward_mean: -0.010730242417047812, reward_median: -0.014500000000000002, reward_std: 0.011560113318939493, reward_max: 0.04611111111111111, reward_min: -0.028000000000000004, runs: 2000
2019-04-08 10:17:04,248 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:17:56,889 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:18:15,917 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:18:15,925 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:19:07,752 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:19:26,759 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:19:26,767 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:20:18,621 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:20:37,434 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:20:37,442 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:21:30,500 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:21:49,620 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:21:49,628 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:22:39,497 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:22:58,440 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:22:58,449 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:23:48,043 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:24:06,958 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:24:06,966 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:24:26,966 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:24:33,574 - experiments.base - INFO - reward_mean: -0.001362957025065411, reward_median: -0.002887323943661972, reward_std: 0.005428838832366879, reward_max: 0.028846153846153848, reward_min: -0.008638814016172506, runs: 2000
2019-04-08 10:24:33,594 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:24:53,253 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:24:59,587 - experiments.base - INFO - reward_mean: -0.0001467127957260793, reward_median: -0.0017213114754098357, reward_std: 0.006264602304764769, reward_max: 0.033913043478260865, reward_min: -0.008749999999999997, runs: 2000
2019-04-08 10:24:59,605 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:25:17,708 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:25:24,182 - experiments.base - INFO - reward_mean: -0.001064173984946377, reward_median: -0.002627737226277372, reward_std: 0.005431254896869658, reward_max: 0.03208333333333333, reward_min: -0.008476621417797887, runs: 2000
2019-04-08 10:25:24,201 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:25:34,653 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:25:40,182 - experiments.base - INFO - reward_mean: 0.0020638238323547843, reward_median: 0.0010989010989011, reward_std: 0.008454406246178785, reward_max: 0.04611111111111111, reward_min: -0.014090909090909093, runs: 2000
2019-04-08 10:25:40,201 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:26:31,611 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:26:50,576 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:26:50,584 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:27:40,255 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:27:59,179 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:27:59,187 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:28:48,181 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:29:07,001 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:29:07,010 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:29:56,021 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:30:14,919 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:30:14,927 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:31:01,850 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:31:21,090 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:31:21,099 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:32:08,357 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:32:26,949 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:32:26,958 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:32:47,790 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:33:06,643 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:33:06,651 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:33:26,998 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:33:34,254 - experiments.base - INFO - reward_mean: -0.0024289134858803026, reward_median: -0.004261363636363635, reward_std: 0.005850999402675935, reward_max: 0.03809523809523809, reward_min: -0.01, runs: 2000
2019-04-08 10:33:34,272 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:33:51,104 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:33:57,518 - experiments.base - INFO - reward_mean: -0.0007032821920806407, reward_median: -0.002348484848484849, reward_std: 0.006136791781150076, reward_max: 0.03809523809523809, reward_min: -0.008916309012875535, runs: 2000
2019-04-08 10:33:57,537 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:34:06,876 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:34:12,355 - experiments.base - INFO - reward_mean: -0.00018239680719191968, reward_median: -0.000733944954128439, reward_std: 0.009468793626741204, reward_max: 0.0431578947368421, reward_min: -0.017499999999999998, runs: 2000
2019-04-08 10:34:12,373 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:35:00,905 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:35:19,605 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:35:19,614 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:36:07,044 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:36:26,010 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:36:26,020 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:37:14,190 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:37:32,911 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:37:32,919 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:38:19,851 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:38:38,745 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:38:38,754 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:39:24,610 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:39:43,178 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:39:43,186 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:40:28,069 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:40:46,805 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:40:46,813 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:41:20,526 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:41:39,707 - experiments.base - INFO - reward_mean: -0.01, reward_median: -0.01, reward_std: 0.0, reward_max: -0.01, reward_min: -0.01, runs: 2000
2019-04-08 10:41:39,716 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:41:54,821 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:42:00,844 - experiments.base - INFO - reward_mean: 0.0006200776287036323, reward_median: -0.000733944954128439, reward_std: 0.006100097189029525, reward_max: 0.03809523809523809, reward_min: -0.00870346598202824, runs: 2000
2019-04-08 10:42:00,862 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:42:15,297 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:42:21,291 - experiments.base - INFO - reward_mean: 0.0009980910891616947, reward_median: -0.0009009009009008997, reward_std: 0.006803129606278262, reward_max: 0.03809523809523809, reward_min: -0.00830820770519263, runs: 2000
2019-04-08 10:42:21,309 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:42:31,145 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:42:37,128 - experiments.base - INFO - reward_mean: -0.0022037968738009006, reward_median: -0.003129251700680272, reward_std: 0.007830923723388162, reward_max: 0.040499999999999994, reward_min: -0.022857142857142857, runs: 2000
2019-04-08 10:42:37,148 - __main__ - INFO - Running Q experiment: Cliff Walking (4x12)
2019-04-08 10:42:37,148 - experiments.base - INFO - Searching Q in 180 dimensions
2019-04-08 10:42:37,149 - experiments.base - INFO - 1/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:43:31,218 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:43:53,668 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 10:43:53,673 - experiments.base - INFO - 2/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:43:56,380 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:44:11,530 - experiments.base - INFO - reward_mean: -3.2021634615384613, reward_median: 6.769230769230769, reward_std: 10.07161176625538, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:44:11,544 - experiments.base - INFO - 3/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:44:14,030 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:44:29,331 - experiments.base - INFO - reward_mean: -3.3431730769230765, reward_median: -13.375, reward_std: 10.07203480737, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:44:29,344 - experiments.base - INFO - 4/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:44:31,729 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:44:46,986 - experiments.base - INFO - reward_mean: -3.484182692307692, reward_median: -13.375, reward_std: 10.070483569735362, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:44:46,999 - experiments.base - INFO - 5/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:44:49,342 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:45:04,269 - experiments.base - INFO - reward_mean: -3.171947115384615, reward_median: 6.769230769230769, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:45:04,284 - experiments.base - INFO - 6/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:45:06,596 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:45:21,773 - experiments.base - INFO - reward_mean: -2.9604326923076925, reward_median: 6.769230769230769, reward_std: 10.06629201848361, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:45:21,786 - experiments.base - INFO - 7/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:45:24,079 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:45:39,137 - experiments.base - INFO - reward_mean: -3.252524038461538, reward_median: 6.769230769230769, reward_std: 10.071989482386185, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:45:39,151 - experiments.base - INFO - 8/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:45:41,387 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:45:56,595 - experiments.base - INFO - reward_mean: -3.3331009615384612, reward_median: -13.375, reward_std: 10.072070059994173, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:45:56,609 - experiments.base - INFO - 9/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:45:58,843 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:46:14,250 - experiments.base - INFO - reward_mean: -3.5043269230769227, reward_median: -13.375, reward_std: 10.070100760055855, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:46:14,368 - experiments.base - INFO - 10/180 Processing Q with alpha 0.1, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:46:16,581 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:46:31,835 - experiments.base - INFO - reward_mean: -3.3834615384615385, reward_median: -13.375, reward_std: 10.071793071765988, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:46:31,848 - experiments.base - INFO - 11/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:47:27,823 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:47:50,405 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 10:47:50,410 - experiments.base - INFO - 12/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:47:53,212 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:48:08,410 - experiments.base - INFO - reward_mean: -3.121586538461538, reward_median: 6.769230769230769, reward_std: 10.070483569735362, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:48:08,424 - experiments.base - INFO - 13/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:48:11,037 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:48:26,636 - experiments.base - INFO - reward_mean: -3.5546875, reward_median: -13.375, reward_std: 10.068967356601435, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:48:26,648 - experiments.base - INFO - 14/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:48:29,187 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:48:44,275 - experiments.base - INFO - reward_mean: -3.1417307692307697, reward_median: 6.769230769230769, reward_std: 10.070826071324824, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:48:44,289 - experiments.base - INFO - 15/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:48:46,752 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:49:02,415 - experiments.base - INFO - reward_mean: -3.574831730769231, reward_median: -13.375, reward_std: 10.068443429221814, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:49:02,429 - experiments.base - INFO - 16/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:49:04,899 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:49:19,976 - experiments.base - INFO - reward_mean: -3.1316586538461544, reward_median: 6.769230769230769, reward_std: 10.070659858772968, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:49:19,990 - experiments.base - INFO - 17/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:49:22,424 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:49:38,169 - experiments.base - INFO - reward_mean: -3.4539663461538463, reward_median: -13.375, reward_std: 10.070982207889838, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:49:38,182 - experiments.base - INFO - 18/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:49:40,576 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:49:55,778 - experiments.base - INFO - reward_mean: -3.323028846153846, reward_median: -13.375, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:49:55,791 - experiments.base - INFO - 19/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:49:58,183 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:50:13,413 - experiments.base - INFO - reward_mean: -3.2223076923076923, reward_median: 6.769230769230769, reward_std: 10.071793071765988, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:50:13,427 - experiments.base - INFO - 20/180 Processing Q with alpha 0.1, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:50:15,839 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:50:30,834 - experiments.base - INFO - reward_mean: -3.1014423076923077, reward_median: 6.769230769230769, reward_std: 10.070100760055855, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:50:30,949 - experiments.base - INFO - 21/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:51:25,022 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:51:47,316 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 10:51:47,321 - experiments.base - INFO - 22/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:51:50,190 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:52:05,968 - experiments.base - INFO - reward_mean: -3.262596153846154, reward_median: 6.769230769230769, reward_std: 10.072034807369999, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:52:05,984 - experiments.base - INFO - 23/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:52:09,385 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:52:24,856 - experiments.base - INFO - reward_mean: -3.4338221153846153, reward_median: -13.375, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:52:24,869 - experiments.base - INFO - 24/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:52:27,546 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:52:42,606 - experiments.base - INFO - reward_mean: -3.141730769230769, reward_median: 6.769230769230769, reward_std: 10.070826071324824, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:52:42,619 - experiments.base - INFO - 25/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:52:45,191 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:53:00,196 - experiments.base - INFO - reward_mean: -3.2122355769230766, reward_median: 6.769230769230769, reward_std: 10.071707455681581, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:53:00,208 - experiments.base - INFO - 26/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:53:02,780 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:53:17,448 - experiments.base - INFO - reward_mean: -2.5071875, reward_median: 6.769230769230769, reward_std: 10.040636156219112, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:53:17,464 - experiments.base - INFO - 27/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:53:20,038 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:53:35,316 - experiments.base - INFO - reward_mean: -3.625192307692308, reward_median: -13.375, reward_std: 10.066957140689793, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:53:35,328 - experiments.base - INFO - 28/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:53:37,854 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:53:53,209 - experiments.base - INFO - reward_mean: -3.3834615384615385, reward_median: -13.375, reward_std: 10.071793071765988, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:53:53,222 - experiments.base - INFO - 29/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:53:55,864 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:54:11,016 - experiments.base - INFO - reward_mean: -2.5001666666666664, reward_median: 5.733333333333333, reward_std: 8.316250822936981, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 10:54:11,028 - experiments.base - INFO - 30/180 Processing Q with alpha 0.1, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:54:13,584 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:54:29,112 - experiments.base - INFO - reward_mean: -3.574831730769231, reward_median: -13.375, reward_std: 10.068443429221812, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:54:29,125 - experiments.base - INFO - 31/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:55:23,549 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:55:45,896 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 10:55:46,006 - experiments.base - INFO - 32/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:55:48,683 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:56:03,862 - experiments.base - INFO - reward_mean: -3.393533653846154, reward_median: -13.375, reward_std: 10.07170745568158, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:56:03,875 - experiments.base - INFO - 33/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 10:56:06,396 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:56:21,505 - experiments.base - INFO - reward_mean: -3.141730769230769, reward_median: 6.769230769230769, reward_std: 10.070826071324824, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:56:21,520 - experiments.base - INFO - 34/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 10:56:23,910 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:56:39,143 - experiments.base - INFO - reward_mean: -3.343173076923077, reward_median: -13.375, reward_std: 10.072034807369999, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:56:39,156 - experiments.base - INFO - 35/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 10:56:41,480 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:56:56,953 - experiments.base - INFO - reward_mean: -3.4942548076923075, reward_median: -13.375, reward_std: 10.070297203682818, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:56:56,968 - experiments.base - INFO - 36/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 10:56:59,258 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:57:14,971 - experiments.base - INFO - reward_mean: -3.071225961538462, reward_median: 6.769230769230769, reward_std: 10.069450957679079, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:57:14,988 - experiments.base - INFO - 37/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 10:57:17,940 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:57:33,676 - experiments.base - INFO - reward_mean: -3.343173076923077, reward_median: -13.375, reward_std: 10.072034807369999, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:57:33,688 - experiments.base - INFO - 38/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 10:57:35,929 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:57:51,062 - experiments.base - INFO - reward_mean: -3.4036057692307695, reward_median: -13.375, reward_std: 10.07161176625538, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:57:51,075 - experiments.base - INFO - 39/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 10:57:53,324 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:58:08,833 - experiments.base - INFO - reward_mean: -3.5345432692307694, reward_median: -13.375, reward_std: 10.069450957679077, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:58:08,845 - experiments.base - INFO - 40/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 10:58:11,039 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:58:26,153 - experiments.base - INFO - reward_mean: -3.4338221153846153, reward_median: -13.375, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 10:58:26,165 - experiments.base - INFO - 41/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 10:59:20,275 - experiments.base - INFO - Took 2000 episodes
2019-04-08 10:59:42,543 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 10:59:42,548 - experiments.base - INFO - 42/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 10:59:45,347 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:00:00,272 - experiments.base - INFO - reward_mean: -2.859711538461538, reward_median: 6.769230769230769, reward_std: 10.062360853442717, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:00:00,376 - experiments.base - INFO - 43/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:00:02,978 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:00:18,240 - experiments.base - INFO - reward_mean: -3.5244711538461533, reward_median: -13.375, reward_std: 10.06967763768923, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:00:18,252 - experiments.base - INFO - 44/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:00:20,824 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:00:36,224 - experiments.base - INFO - reward_mean: -3.4942548076923075, reward_median: -13.375, reward_std: 10.070297203682818, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:00:36,237 - experiments.base - INFO - 45/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:00:38,717 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:00:53,902 - experiments.base - INFO - reward_mean: -3.3028846153846154, reward_median: -3.3028846153846154, reward_std: 10.072115384615385, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:00:53,914 - experiments.base - INFO - 46/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:00:56,328 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:01:11,650 - experiments.base - INFO - reward_mean: -3.373389423076923, reward_median: -13.375, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:01:11,666 - experiments.base - INFO - 47/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:01:14,167 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:01:29,261 - experiments.base - INFO - reward_mean: -3.161875, reward_median: 6.769230769230769, reward_std: 10.07112826893665, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:01:29,275 - experiments.base - INFO - 48/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:01:31,723 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:01:46,897 - experiments.base - INFO - reward_mean: -3.2122355769230766, reward_median: 6.769230769230769, reward_std: 10.071707455681581, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:01:46,910 - experiments.base - INFO - 49/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:01:49,300 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:02:04,390 - experiments.base - INFO - reward_mean: -3.2323798076923076, reward_median: 6.769230769230769, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:02:04,404 - experiments.base - INFO - 50/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:02:06,774 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:02:22,356 - experiments.base - INFO - reward_mean: -3.5546875, reward_median: -13.375, reward_std: 10.068967356601435, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:02:22,370 - experiments.base - INFO - 51/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:03:18,110 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:03:40,504 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:03:40,509 - experiments.base - INFO - 52/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:03:43,362 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:03:58,626 - experiments.base - INFO - reward_mean: -3.3532451923076922, reward_median: -13.375, reward_std: 10.071989482386185, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:03:58,638 - experiments.base - INFO - 53/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:04:01,327 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:04:16,326 - experiments.base - INFO - reward_mean: -2.9402884615384615, reward_median: 6.769230769230769, reward_std: 10.065586537813973, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:04:16,438 - experiments.base - INFO - 54/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:04:19,022 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:04:34,331 - experiments.base - INFO - reward_mean: -3.433822115384616, reward_median: -13.375, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:04:34,345 - experiments.base - INFO - 55/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:04:37,015 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:04:52,145 - experiments.base - INFO - reward_mean: -3.2323798076923076, reward_median: 6.769230769230769, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:04:52,158 - experiments.base - INFO - 56/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:04:54,743 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:05:10,153 - experiments.base - INFO - reward_mean: -3.5244711538461533, reward_median: -13.375, reward_std: 10.06967763768923, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:05:10,166 - experiments.base - INFO - 57/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:05:12,722 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:05:28,222 - experiments.base - INFO - reward_mean: -3.594975961538462, reward_median: -13.375, reward_std: 10.06787916924453, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:05:28,234 - experiments.base - INFO - 58/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:05:30,798 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:05:46,039 - experiments.base - INFO - reward_mean: -3.443894230769231, reward_median: -13.375, reward_std: 10.071128268936652, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:05:46,051 - experiments.base - INFO - 59/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:05:48,635 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:06:03,940 - experiments.base - INFO - reward_mean: -3.4338221153846153, reward_median: -13.375, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:06:03,952 - experiments.base - INFO - 60/180 Processing Q with alpha 0.1, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:06:06,566 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:06:21,614 - experiments.base - INFO - reward_mean: -3.151802884615385, reward_median: 6.769230769230769, reward_std: 10.070982207889838, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:06:21,628 - experiments.base - INFO - 61/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:07:17,025 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:07:40,422 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:07:40,427 - experiments.base - INFO - 62/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:07:42,762 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:07:58,032 - experiments.base - INFO - reward_mean: -3.2726682692307696, reward_median: 6.769230769230769, reward_std: 10.072070059994173, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:07:58,044 - experiments.base - INFO - 63/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:08:00,181 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:08:15,386 - experiments.base - INFO - reward_mean: -3.2021634615384618, reward_median: 6.769230769230769, reward_std: 10.07161176625538, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:08:15,400 - experiments.base - INFO - 64/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:08:17,711 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:08:32,814 - experiments.base - INFO - reward_mean: -2.5334333333333334, reward_median: 5.733333333333333, reward_std: 8.316516965319344, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:08:32,917 - experiments.base - INFO - 65/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:08:35,077 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:08:50,410 - experiments.base - INFO - reward_mean: -3.3129567307692307, reward_median: -13.375, reward_std: 10.072110348556432, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:08:50,422 - experiments.base - INFO - 66/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:08:52,563 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:09:07,915 - experiments.base - INFO - reward_mean: -3.4841826923076917, reward_median: -13.375, reward_std: 10.070483569735362, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:09:07,928 - experiments.base - INFO - 67/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:09:10,123 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:09:25,158 - experiments.base - INFO - reward_mean: -3.081298076923077, reward_median: 6.769230769230769, reward_std: 10.06967763768923, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:09:25,171 - experiments.base - INFO - 68/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:09:27,482 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:09:42,526 - experiments.base - INFO - reward_mean: -2.3171999999999997, reward_median: 5.733333333333333, reward_std: 8.312407442692722, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:09:42,539 - experiments.base - INFO - 69/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:09:44,814 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:10:00,115 - experiments.base - INFO - reward_mean: -2.6415499999999996, reward_median: -10.9, reward_std: 8.316462905837232, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:10:00,127 - experiments.base - INFO - 70/180 Processing Q with alpha 0.5, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:10:02,271 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:10:17,988 - experiments.base - INFO - reward_mean: -3.7057692307692305, reward_median: -13.375, reward_std: 10.064054466649727, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:10:18,000 - experiments.base - INFO - 71/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:11:12,979 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:11:35,314 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:11:35,319 - experiments.base - INFO - 72/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:11:37,661 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:11:52,701 - experiments.base - INFO - reward_mean: -3.0007211538461545, reward_median: 6.769230769230769, reward_std: 10.067581912431457, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:11:52,714 - experiments.base - INFO - 73/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:11:55,065 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:12:10,119 - experiments.base - INFO - reward_mean: -3.202163461538461, reward_median: 6.769230769230769, reward_std: 10.071611766255378, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:12:10,131 - experiments.base - INFO - 74/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:12:12,464 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:12:27,545 - experiments.base - INFO - reward_mean: -3.0712259615384614, reward_median: 6.769230769230769, reward_std: 10.069450957679077, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:12:27,558 - experiments.base - INFO - 75/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:12:29,883 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:12:45,468 - experiments.base - INFO - reward_mean: -3.0309375, reward_median: 6.769230769230769, reward_std: 10.068443429221814, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:12:45,572 - experiments.base - INFO - 76/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:12:47,985 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:13:03,575 - experiments.base - INFO - reward_mean: -3.2323798076923076, reward_median: 6.769230769230769, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:13:03,589 - experiments.base - INFO - 77/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:13:06,026 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:13:21,533 - experiments.base - INFO - reward_mean: -3.6755528846153847, reward_median: -13.375, reward_std: 10.065218660422358, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:13:21,547 - experiments.base - INFO - 78/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:13:23,870 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:13:39,027 - experiments.base - INFO - reward_mean: -3.2928125, reward_median: 6.769230769230769, reward_std: 10.072110348556432, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:13:39,039 - experiments.base - INFO - 79/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:13:41,499 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:13:56,635 - experiments.base - INFO - reward_mean: -2.3754166666666667, reward_median: 5.733333333333333, reward_std: 8.314067302119142, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:13:56,649 - experiments.base - INFO - 80/180 Processing Q with alpha 0.5, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:13:58,978 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:14:14,405 - experiments.base - INFO - reward_mean: -3.6151201923076925, reward_median: -13.375, reward_std: 10.067274569887795, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:14:14,417 - experiments.base - INFO - 81/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:15:09,547 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:15:32,009 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:15:32,014 - experiments.base - INFO - 82/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:15:34,551 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:15:49,570 - experiments.base - INFO - reward_mean: -3.2323798076923076, reward_median: 6.769230769230769, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:15:49,583 - experiments.base - INFO - 83/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:15:52,099 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:16:07,433 - experiments.base - INFO - reward_mean: -3.474110576923077, reward_median: -13.375, reward_std: 10.070659858772967, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:16:07,447 - experiments.base - INFO - 84/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:16:09,955 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:16:25,365 - experiments.base - INFO - reward_mean: -3.6050480769230773, reward_median: -13.375, reward_std: 10.067581912431457, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:16:25,378 - experiments.base - INFO - 85/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:16:27,861 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:16:42,726 - experiments.base - INFO - reward_mean: -3.000721153846154, reward_median: 6.769230769230769, reward_std: 10.067581912431457, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:16:42,740 - experiments.base - INFO - 86/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:16:45,193 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:17:00,114 - experiments.base - INFO - reward_mean: -2.950360576923077, reward_median: 6.769230769230769, reward_std: 10.065944323475701, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:17:00,219 - experiments.base - INFO - 87/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:17:02,732 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:17:17,936 - experiments.base - INFO - reward_mean: -3.645336538461538, reward_median: -13.375, reward_std: 10.06629201848361, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:17:17,948 - experiments.base - INFO - 88/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:17:20,520 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:17:35,399 - experiments.base - INFO - reward_mean: 2.511433333333333, reward_median: 5.733333333333333, reward_std: 3.3635527432562533, reward_max: 5.733333333333333, reward_min: -1.0, runs: 2000
2019-04-08 11:17:35,411 - experiments.base - INFO - 89/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:17:37,953 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:17:53,480 - experiments.base - INFO - reward_mean: -2.54175, reward_median: 5.733333333333333, reward_std: 8.316562707683586, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:17:53,492 - experiments.base - INFO - 90/180 Processing Q with alpha 0.5, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:17:56,019 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:18:11,902 - experiments.base - INFO - reward_mean: -3.3028846153846154, reward_median: -3.3028846153846154, reward_std: 10.072115384615385, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:18:11,915 - experiments.base - INFO - 91/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:19:06,753 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:19:29,071 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:19:29,076 - experiments.base - INFO - 92/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:19:31,267 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:19:46,246 - experiments.base - INFO - reward_mean: -3.131658653846154, reward_median: 6.769230769230769, reward_std: 10.070659858772967, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:19:46,260 - experiments.base - INFO - 93/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:19:48,422 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:20:03,484 - experiments.base - INFO - reward_mean: -3.2122355769230766, reward_median: 6.769230769230769, reward_std: 10.071707455681581, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:20:03,499 - experiments.base - INFO - 94/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:20:05,631 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:20:20,842 - experiments.base - INFO - reward_mean: -3.192091346153846, reward_median: 6.769230769230769, reward_std: 10.07150600320027, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:20:20,855 - experiments.base - INFO - 95/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:20:22,960 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:20:38,166 - experiments.base - INFO - reward_mean: -3.2323798076923076, reward_median: 6.769230769230769, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:20:38,179 - experiments.base - INFO - 96/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:20:40,310 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:20:55,560 - experiments.base - INFO - reward_mean: -3.2021634615384613, reward_median: 6.769230769230769, reward_std: 10.07161176625538, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:20:55,573 - experiments.base - INFO - 97/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:20:57,685 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:21:13,056 - experiments.base - INFO - reward_mean: -3.171947115384615, reward_median: 6.769230769230769, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:21:13,167 - experiments.base - INFO - 98/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:21:15,448 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:21:30,923 - experiments.base - INFO - reward_mean: 2.2892333333333337, reward_median: -1.0, reward_std: 3.3657760655357527, reward_max: 5.733333333333333, reward_min: -1.0, runs: 2000
2019-04-08 11:21:30,934 - experiments.base - INFO - 99/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:21:33,053 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:21:48,061 - experiments.base - INFO - reward_mean: -3.0712259615384614, reward_median: 6.769230769230769, reward_std: 10.069450957679077, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:21:48,074 - experiments.base - INFO - 100/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:21:50,369 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:22:05,749 - experiments.base - INFO - reward_mean: -2.558383333333333, reward_median: 5.733333333333333, reward_std: 8.316629241582458, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:22:05,762 - experiments.base - INFO - 101/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:23:01,131 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:23:24,360 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:23:24,365 - experiments.base - INFO - 102/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:23:26,772 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:23:42,000 - experiments.base - INFO - reward_mean: -3.3230288461538455, reward_median: -13.375, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:23:42,013 - experiments.base - INFO - 103/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:23:44,373 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:23:59,530 - experiments.base - INFO - reward_mean: -3.3331009615384612, reward_median: -13.375, reward_std: 10.072070059994173, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:23:59,542 - experiments.base - INFO - 104/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:24:01,859 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:24:16,974 - experiments.base - INFO - reward_mean: -3.1820192307692303, reward_median: 6.769230769230769, reward_std: 10.071390166198888, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:24:16,987 - experiments.base - INFO - 105/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:24:19,338 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:24:34,523 - experiments.base - INFO - reward_mean: -3.111514423076923, reward_median: 6.769230769230769, reward_std: 10.070297203682818, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:24:34,536 - experiments.base - INFO - 106/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:24:36,986 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:24:52,348 - experiments.base - INFO - reward_mean: 2.3430999999999997, reward_median: -1.0, reward_std: 3.366584182322888, reward_max: 5.733333333333333, reward_min: -1.0, runs: 2000
2019-04-08 11:24:52,360 - experiments.base - INFO - 107/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:24:54,675 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:25:09,982 - experiments.base - INFO - reward_mean: -3.655408653846153, reward_median: -13.375, reward_std: 10.065944323475701, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:25:09,995 - experiments.base - INFO - 108/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:25:12,362 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:25:27,567 - experiments.base - INFO - reward_mean: -3.373389423076923, reward_median: -13.375, reward_std: 10.071868614765494, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:25:27,676 - experiments.base - INFO - 109/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:25:30,032 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:25:45,195 - experiments.base - INFO - reward_mean: -3.3431730769230765, reward_median: -13.375, reward_std: 10.072034807369999, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:25:45,208 - experiments.base - INFO - 110/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:25:47,685 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:26:02,851 - experiments.base - INFO - reward_mean: 2.3464666666666667, reward_median: -1.0, reward_std: 3.3666060661212573, reward_max: 5.733333333333333, reward_min: -1.0, runs: 2000
2019-04-08 11:26:02,863 - experiments.base - INFO - 111/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:26:57,709 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:27:19,931 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:27:19,936 - experiments.base - INFO - 112/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:27:22,426 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:27:37,696 - experiments.base - INFO - reward_mean: -3.474110576923077, reward_median: -13.375, reward_std: 10.070659858772967, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:27:37,709 - experiments.base - INFO - 113/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:27:40,253 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:27:55,271 - experiments.base - INFO - reward_mean: -3.3532451923076922, reward_median: -13.375, reward_std: 10.071989482386185, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:27:55,284 - experiments.base - INFO - 114/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:27:57,818 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:28:13,283 - experiments.base - INFO - reward_mean: -2.950360576923077, reward_median: 6.769230769230769, reward_std: 10.065944323475701, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:28:13,296 - experiments.base - INFO - 115/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:28:15,773 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:28:31,413 - experiments.base - INFO - reward_mean: -3.1820192307692303, reward_median: 6.769230769230769, reward_std: 10.071390166198888, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:28:31,427 - experiments.base - INFO - 116/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:28:33,965 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:28:48,942 - experiments.base - INFO - reward_mean: -2.980576923076923, reward_median: 6.769230769230769, reward_std: 10.066957140689793, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:28:48,955 - experiments.base - INFO - 117/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:28:51,479 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:29:06,958 - experiments.base - INFO - reward_mean: -3.6755528846153847, reward_median: -13.375, reward_std: 10.065218660422358, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:29:06,972 - experiments.base - INFO - 118/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:29:09,466 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:29:24,639 - experiments.base - INFO - reward_mean: -3.1115144230769234, reward_median: 6.769230769230769, reward_std: 10.070297203682818, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:29:24,652 - experiments.base - INFO - 119/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:29:27,216 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:29:42,462 - experiments.base - INFO - reward_mean: -2.6665, reward_median: -10.9, reward_std: 8.316250822936981, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:29:42,564 - experiments.base - INFO - 120/180 Processing Q with alpha 0.5, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:29:45,048 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:30:00,178 - experiments.base - INFO - reward_mean: -3.262596153846154, reward_median: 6.769230769230769, reward_std: 10.072034807369999, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:30:00,190 - experiments.base - INFO - 121/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:30:55,517 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:31:18,055 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:31:18,060 - experiments.base - INFO - 122/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:31:20,181 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:31:35,326 - experiments.base - INFO - reward_mean: -3.1820192307692308, reward_median: 6.769230769230769, reward_std: 10.071390166198888, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:31:35,339 - experiments.base - INFO - 123/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:31:37,488 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:31:52,998 - experiments.base - INFO - reward_mean: -3.242451923076923, reward_median: 6.769230769230769, reward_std: 10.071934084906749, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:31:53,012 - experiments.base - INFO - 124/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:31:55,103 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:32:10,533 - experiments.base - INFO - reward_mean: -3.453966346153847, reward_median: -13.375, reward_std: 10.070982207889838, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:32:10,545 - experiments.base - INFO - 125/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:32:12,675 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:32:28,102 - experiments.base - INFO - reward_mean: -3.5546875, reward_median: -13.375, reward_std: 10.068967356601435, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:32:28,114 - experiments.base - INFO - 126/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:32:30,209 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:32:45,632 - experiments.base - INFO - reward_mean: -3.5244711538461537, reward_median: -13.375, reward_std: 10.06967763768923, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:32:45,645 - experiments.base - INFO - 127/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:32:47,799 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:33:02,972 - experiments.base - INFO - reward_mean: -3.161875, reward_median: 6.769230769230769, reward_std: 10.071128268936652, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:33:02,986 - experiments.base - INFO - 128/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:33:05,086 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:33:21,177 - experiments.base - INFO - reward_mean: -3.594975961538461, reward_median: -13.375, reward_std: 10.067879169244529, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:33:21,190 - experiments.base - INFO - 129/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:33:23,288 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:33:38,925 - experiments.base - INFO - reward_mean: -3.2021634615384613, reward_median: 6.769230769230769, reward_std: 10.07161176625538, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:33:38,939 - experiments.base - INFO - 130/180 Processing Q with alpha 0.9, q_init random, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:33:41,187 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:33:56,663 - experiments.base - INFO - reward_mean: -3.1417307692307697, reward_median: 6.769230769230769, reward_std: 10.070826071324824, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:33:56,771 - experiments.base - INFO - 131/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:34:51,533 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:35:13,969 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:35:13,973 - experiments.base - INFO - 132/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:35:16,301 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:35:31,453 - experiments.base - INFO - reward_mean: -3.1417307692307697, reward_median: 6.769230769230769, reward_std: 10.070826071324824, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:35:31,466 - experiments.base - INFO - 133/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:35:33,784 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:35:49,017 - experiments.base - INFO - reward_mean: -3.3633173076923075, reward_median: -13.375, reward_std: 10.071934084906749, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:35:49,030 - experiments.base - INFO - 134/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:35:51,326 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:36:06,671 - experiments.base - INFO - reward_mean: -3.252524038461538, reward_median: 6.769230769230769, reward_std: 10.071989482386185, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:36:06,685 - experiments.base - INFO - 135/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:36:08,988 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:36:24,631 - experiments.base - INFO - reward_mean: -3.826634615384615, reward_median: -13.375, reward_std: 10.05848866671857, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:36:24,643 - experiments.base - INFO - 136/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:36:26,934 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:36:42,132 - experiments.base - INFO - reward_mean: -3.2223076923076923, reward_median: 6.769230769230769, reward_std: 10.071793071765988, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:36:42,146 - experiments.base - INFO - 137/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:36:44,478 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:36:59,712 - experiments.base - INFO - reward_mean: -3.514399038461538, reward_median: -13.375, reward_std: 10.069894238264688, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:36:59,725 - experiments.base - INFO - 138/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:37:02,035 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:37:17,576 - experiments.base - INFO - reward_mean: -3.4640384615384616, reward_median: -13.375, reward_std: 10.070826071324822, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:37:17,588 - experiments.base - INFO - 139/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:37:19,917 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:37:35,067 - experiments.base - INFO - reward_mean: -3.3230288461538455, reward_median: -13.375, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:37:35,080 - experiments.base - INFO - 140/180 Processing Q with alpha 0.9, q_init random, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:37:37,470 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:37:52,684 - experiments.base - INFO - reward_mean: -2.3171999999999997, reward_median: 5.733333333333333, reward_std: 8.312407442692722, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:37:52,697 - experiments.base - INFO - 141/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:38:48,618 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:39:11,499 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:39:11,600 - experiments.base - INFO - 142/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:39:14,036 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:39:28,979 - experiments.base - INFO - reward_mean: -3.1115144230769234, reward_median: 6.769230769230769, reward_std: 10.070297203682818, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:39:28,992 - experiments.base - INFO - 143/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:39:31,576 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:39:46,833 - experiments.base - INFO - reward_mean: -3.413677884615385, reward_median: -13.375, reward_std: 10.07150600320027, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:39:46,846 - experiments.base - INFO - 144/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:39:49,381 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:40:04,385 - experiments.base - INFO - reward_mean: -3.1316586538461535, reward_median: 6.769230769230769, reward_std: 10.070659858772967, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:40:04,398 - experiments.base - INFO - 145/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:40:06,909 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:40:22,098 - experiments.base - INFO - reward_mean: -3.0712259615384614, reward_median: 6.769230769230769, reward_std: 10.069450957679077, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:40:22,112 - experiments.base - INFO - 146/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:40:24,610 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:40:39,795 - experiments.base - INFO - reward_mean: -3.393533653846154, reward_median: -13.375, reward_std: 10.07170745568158, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:40:39,809 - experiments.base - INFO - 147/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:40:42,291 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:40:57,609 - experiments.base - INFO - reward_mean: -3.323028846153846, reward_median: -13.375, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:40:57,622 - experiments.base - INFO - 148/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:41:00,080 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:41:15,045 - experiments.base - INFO - reward_mean: -3.151802884615385, reward_median: 6.769230769230769, reward_std: 10.070982207889838, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:41:15,059 - experiments.base - INFO - 149/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:41:17,568 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:41:32,684 - experiments.base - INFO - reward_mean: -3.1417307692307697, reward_median: 6.769230769230769, reward_std: 10.070826071324824, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:41:32,697 - experiments.base - INFO - 150/180 Processing Q with alpha 0.9, q_init random, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:41:35,217 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:41:50,312 - experiments.base - INFO - reward_mean: -3.3230288461538455, reward_median: -13.375, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:41:50,328 - experiments.base - INFO - 151/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:42:45,206 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:43:07,907 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:43:07,912 - experiments.base - INFO - 152/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:43:10,027 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:43:26,194 - experiments.base - INFO - reward_mean: -3.715841346153846, reward_median: -13.375, reward_std: 10.063646210967391, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:43:26,320 - experiments.base - INFO - 153/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:43:28,419 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:43:44,046 - experiments.base - INFO - reward_mean: -3.8669230769230767, reward_median: -13.375, reward_std: 10.056309906467245, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:43:44,059 - experiments.base - INFO - 154/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:43:46,180 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:44:01,855 - experiments.base - INFO - reward_mean: -3.131658653846154, reward_median: 6.769230769230769, reward_std: 10.070659858772967, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:44:01,869 - experiments.base - INFO - 155/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:44:04,134 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:44:19,522 - experiments.base - INFO - reward_mean: -3.081298076923077, reward_median: 6.769230769230769, reward_std: 10.06967763768923, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:44:19,536 - experiments.base - INFO - 156/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:44:21,687 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:44:37,085 - experiments.base - INFO - reward_mean: -3.282740384615385, reward_median: 6.769230769230769, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:44:37,099 - experiments.base - INFO - 157/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:44:39,200 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:44:54,433 - experiments.base - INFO - reward_mean: -3.282740384615385, reward_median: 6.769230769230769, reward_std: 10.072095240364472, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:44:54,446 - experiments.base - INFO - 158/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:44:56,567 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:45:11,766 - experiments.base - INFO - reward_mean: -3.010793269230769, reward_median: 6.769230769230769, reward_std: 10.06787916924453, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:45:11,780 - experiments.base - INFO - 159/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:45:13,909 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:45:29,216 - experiments.base - INFO - reward_mean: -3.3834615384615385, reward_median: -13.375, reward_std: 10.071793071765988, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:45:29,229 - experiments.base - INFO - 160/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.1, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:45:31,551 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:45:47,147 - experiments.base - INFO - reward_mean: -2.7413499999999993, reward_median: -10.9, reward_std: 8.315165372829336, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:45:47,160 - experiments.base - INFO - 161/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:46:42,544 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:47:05,272 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:47:05,277 - experiments.base - INFO - 162/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:47:07,600 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:47:22,941 - experiments.base - INFO - reward_mean: -3.3633173076923075, reward_median: -13.375, reward_std: 10.071934084906749, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:47:22,953 - experiments.base - INFO - 163/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:47:25,306 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:47:40,476 - experiments.base - INFO - reward_mean: -3.0913701923076924, reward_median: 6.769230769230769, reward_std: 10.069894238264686, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:47:40,581 - experiments.base - INFO - 164/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:47:42,898 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:47:58,353 - experiments.base - INFO - reward_mean: -3.5446153846153843, reward_median: -13.375, reward_std: 10.069214197553503, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:47:58,366 - experiments.base - INFO - 165/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:48:00,817 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:48:16,294 - experiments.base - INFO - reward_mean: -2.6665, reward_median: -10.9, reward_std: 8.31625082293698, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:48:16,306 - experiments.base - INFO - 166/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:48:18,759 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:48:34,701 - experiments.base - INFO - reward_mean: -2.7746166666666667, reward_median: -10.9, reward_std: 8.314466617338322, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:48:34,713 - experiments.base - INFO - 167/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:48:37,149 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:48:52,364 - experiments.base - INFO - reward_mean: -2.4336333333333333, reward_median: 5.733333333333333, reward_std: 8.315319257517684, reward_max: 5.733333333333333, reward_min: -10.9, runs: 2000
2019-04-08 11:48:52,378 - experiments.base - INFO - 168/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:48:54,682 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:49:10,280 - experiments.base - INFO - reward_mean: -3.4438942307692306, reward_median: -13.375, reward_std: 10.071128268936652, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:49:10,293 - experiments.base - INFO - 169/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:49:12,762 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:49:28,507 - experiments.base - INFO - reward_mean: -3.474110576923076, reward_median: -13.375, reward_std: 10.070659858772967, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:49:28,521 - experiments.base - INFO - 170/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.3, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:49:30,806 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:49:46,181 - experiments.base - INFO - reward_mean: -3.413677884615385, reward_median: -13.375, reward_std: 10.07150600320027, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:49:46,194 - experiments.base - INFO - 171/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.0
2019-04-08 11:50:42,138 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:51:05,189 - experiments.base - INFO - reward_mean: -1.0, reward_median: -1.0, reward_std: 0.0, reward_max: -1.0, reward_min: -1.0, runs: 2000
2019-04-08 11:51:05,194 - experiments.base - INFO - 172/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.1
2019-04-08 11:51:07,705 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:51:23,199 - experiments.base - INFO - reward_mean: -3.484182692307692, reward_median: -13.375, reward_std: 10.070483569735362, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:51:23,211 - experiments.base - INFO - 173/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.2
2019-04-08 11:51:25,750 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:51:41,554 - experiments.base - INFO - reward_mean: -3.7460576923076925, reward_median: -13.375, reward_std: 10.062360853442716, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:51:41,567 - experiments.base - INFO - 174/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.3
2019-04-08 11:51:44,035 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:51:59,594 - experiments.base - INFO - reward_mean: -3.484182692307692, reward_median: -13.375, reward_std: 10.070483569735362, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:51:59,711 - experiments.base - INFO - 175/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.4
2019-04-08 11:52:02,182 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:52:17,259 - experiments.base - INFO - reward_mean: -3.0107932692307697, reward_median: 6.769230769230769, reward_std: 10.06787916924453, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:52:17,273 - experiments.base - INFO - 176/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.5
2019-04-08 11:52:19,708 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:52:35,116 - experiments.base - INFO - reward_mean: -3.151802884615385, reward_median: 6.769230769230769, reward_std: 10.070982207889838, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:52:35,129 - experiments.base - INFO - 177/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.6
2019-04-08 11:52:37,657 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:52:53,207 - experiments.base - INFO - reward_mean: -3.5446153846153843, reward_median: -13.375, reward_std: 10.069214197553503, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:52:53,220 - experiments.base - INFO - 178/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.7
2019-04-08 11:52:55,707 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:53:11,095 - experiments.base - INFO - reward_mean: -2.9604326923076925, reward_median: 6.769230769230769, reward_std: 10.06629201848361, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:53:11,109 - experiments.base - INFO - 179/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.8
2019-04-08 11:53:13,593 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:53:29,045 - experiments.base - INFO - reward_mean: -3.171947115384615, reward_median: 6.769230769230769, reward_std: 10.071264254903635, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:53:29,060 - experiments.base - INFO - 180/180 Processing Q with alpha 0.9, q_init 0, epsilon 0.5, epsilon_decay 0.0001, discount_factor 0.9
2019-04-08 11:53:31,599 - experiments.base - INFO - Took 2000 episodes
2019-04-08 11:53:47,413 - experiments.base - INFO - reward_mean: -3.0510817307692304, reward_median: 6.769230769230769, reward_std: 10.068967356601435, reward_max: 6.769230769230769, reward_min: -13.375, runs: 2000
2019-04-08 11:53:47,431 - __main__ - INFO - {'Q': 21380}
